{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "A-sal8nuy3Et"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2d506aca4bb7406d8c825dbfdc292ca3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1bf84388af734749867ab2104b359d4d",
              "IPY_MODEL_3eea2f141a284c8aad3ad9bead6c4d16",
              "IPY_MODEL_b4ba8f4fc0ac45248de005d8d04820ea"
            ],
            "layout": "IPY_MODEL_332a322e4ec447f697cd5295b4a41b6d"
          }
        },
        "1bf84388af734749867ab2104b359d4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9e24cde3a4924044a8efd72e14db7de8",
            "placeholder": "​",
            "style": "IPY_MODEL_092eae81e49e4bbdbae4024b2de45ac9",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "3eea2f141a284c8aad3ad9bead6c4d16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c452a454d074d419024e4b2bc4e1a71",
            "max": 82,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_48b6da49b8294cf1a53294aca79bbd62",
            "value": 82
          }
        },
        "b4ba8f4fc0ac45248de005d8d04820ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2add11e14c3466997d84fa9a87465ca",
            "placeholder": "​",
            "style": "IPY_MODEL_b17c8106bf844c9692080149f9a44303",
            "value": " 82.0/82.0 [00:00&lt;00:00, 8.53kB/s]"
          }
        },
        "332a322e4ec447f697cd5295b4a41b6d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e24cde3a4924044a8efd72e14db7de8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "092eae81e49e4bbdbae4024b2de45ac9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8c452a454d074d419024e4b2bc4e1a71": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48b6da49b8294cf1a53294aca79bbd62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b2add11e14c3466997d84fa9a87465ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b17c8106bf844c9692080149f9a44303": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f9bb05f5010d46fc909ac35b340801c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_acbdf1b496cf487499cb5d18c7996635",
              "IPY_MODEL_6c159068f6bb4527b1e35e913bd0605a",
              "IPY_MODEL_dda0766bc92a4441bf3c48275cab39ed"
            ],
            "layout": "IPY_MODEL_72b73e5f07374d22ad7f42fea00e1a05"
          }
        },
        "acbdf1b496cf487499cb5d18c7996635": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d9e471614cbc41bebc9bdba13769577d",
            "placeholder": "​",
            "style": "IPY_MODEL_24fe334c1728406cb3230f46b8f93367",
            "value": "config.json: 100%"
          }
        },
        "6c159068f6bb4527b1e35e913bd0605a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c60f34b3bb494f2e875b27740fd2ec77",
            "max": 553,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2f54426f0f2c4beca921b58ded452bfb",
            "value": 553
          }
        },
        "dda0766bc92a4441bf3c48275cab39ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d0477e3e3d8f49f5a3305d861d58fea9",
            "placeholder": "​",
            "style": "IPY_MODEL_37a1d6267ce1438996424f5193458cc2",
            "value": " 553/553 [00:00&lt;00:00, 57.3kB/s]"
          }
        },
        "72b73e5f07374d22ad7f42fea00e1a05": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9e471614cbc41bebc9bdba13769577d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24fe334c1728406cb3230f46b8f93367": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c60f34b3bb494f2e875b27740fd2ec77": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f54426f0f2c4beca921b58ded452bfb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d0477e3e3d8f49f5a3305d861d58fea9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37a1d6267ce1438996424f5193458cc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d0f7f41100d847c4a074edf2d37d7be6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_aa6157fe2ee3490eb330f23f1fe03825",
              "IPY_MODEL_df8a9381dec94dfc83aa897cf749ccdb",
              "IPY_MODEL_a48443accfb74d658867bdd4b3f084b2"
            ],
            "layout": "IPY_MODEL_2e7d49e5018c4205b32060cfeee4148a"
          }
        },
        "aa6157fe2ee3490eb330f23f1fe03825": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f74e4af0fdce49dcb03d81ef02df242a",
            "placeholder": "​",
            "style": "IPY_MODEL_7fcdc7fd62ce48159d6177d6b7b50ab8",
            "value": "spiece.model: 100%"
          }
        },
        "df8a9381dec94dfc83aa897cf749ccdb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c485aa1984d46ef8874eb64790222d7",
            "max": 4309802,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1c4b580c9b8c4d7b945d26acbc7e523f",
            "value": 4309802
          }
        },
        "a48443accfb74d658867bdd4b3f084b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dba5e54cee924bff862a79c8ef52b6c4",
            "placeholder": "​",
            "style": "IPY_MODEL_5dea6296362f4bbfa4ef3926ca57cb96",
            "value": " 4.31M/4.31M [00:00&lt;00:00, 10.4MB/s]"
          }
        },
        "2e7d49e5018c4205b32060cfeee4148a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f74e4af0fdce49dcb03d81ef02df242a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7fcdc7fd62ce48159d6177d6b7b50ab8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4c485aa1984d46ef8874eb64790222d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c4b580c9b8c4d7b945d26acbc7e523f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dba5e54cee924bff862a79c8ef52b6c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5dea6296362f4bbfa4ef3926ca57cb96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "88a404797f864a9eafd85c26fc828639": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6fe543d518d34210aba6fa9bd80eb72a",
              "IPY_MODEL_d68f30c01ad64e258f0f12c50399c5a2",
              "IPY_MODEL_b7245f02e1424ac6996884e0c803c4cd"
            ],
            "layout": "IPY_MODEL_6c3ef15f56494a219d350944a5109551"
          }
        },
        "6fe543d518d34210aba6fa9bd80eb72a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d1e24110367044f7907abb7b91b7d30d",
            "placeholder": "​",
            "style": "IPY_MODEL_e86361475dd14ed98485848f45bf3090",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "d68f30c01ad64e258f0f12c50399c5a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_086ef8b5209e4b6980842c682ca2a8ac",
            "max": 99,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_af227bcd42ce42ed9b37387aa9ae30b9",
            "value": 99
          }
        },
        "b7245f02e1424ac6996884e0c803c4cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b8c4e353f57454d954430ab4ed35054",
            "placeholder": "​",
            "style": "IPY_MODEL_be3d775634ee46fca6fbe7c910979a77",
            "value": " 99.0/99.0 [00:00&lt;00:00, 7.81kB/s]"
          }
        },
        "6c3ef15f56494a219d350944a5109551": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1e24110367044f7907abb7b91b7d30d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e86361475dd14ed98485848f45bf3090": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "086ef8b5209e4b6980842c682ca2a8ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af227bcd42ce42ed9b37387aa9ae30b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6b8c4e353f57454d954430ab4ed35054": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be3d775634ee46fca6fbe7c910979a77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a61afdc8d24d4bf79d7844785833bfc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_742f796da73844f5836b74906469310b",
              "IPY_MODEL_58b63b9a7e5546f2af492d0d8547a068",
              "IPY_MODEL_19ba1c28387c4a88854fc6b7d432f0ad"
            ],
            "layout": "IPY_MODEL_f04152c2d6a54a23a7c6b5275b3a33b3"
          }
        },
        "742f796da73844f5836b74906469310b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9e558f77dd3749ddb858a05377995301",
            "placeholder": "​",
            "style": "IPY_MODEL_e66ee4cdfc144d4ebc332addb5145231",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "58b63b9a7e5546f2af492d0d8547a068": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb3a9f7071504273a08920b1ee8ae52f",
            "max": 1200794589,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f42a7e92315a4693b5c2fb13fc41346a",
            "value": 1200794589
          }
        },
        "19ba1c28387c4a88854fc6b7d432f0ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8dd613aa75224e4fa2028c88d617ba80",
            "placeholder": "​",
            "style": "IPY_MODEL_ca1011769b2c4b01b7b296e82a39cfc2",
            "value": " 1.20G/1.20G [00:04&lt;00:00, 256MB/s]"
          }
        },
        "f04152c2d6a54a23a7c6b5275b3a33b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e558f77dd3749ddb858a05377995301": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e66ee4cdfc144d4ebc332addb5145231": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cb3a9f7071504273a08920b1ee8ae52f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f42a7e92315a4693b5c2fb13fc41346a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8dd613aa75224e4fa2028c88d617ba80": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca1011769b2c4b01b7b296e82a39cfc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a9dbe6f4e50c4693a65b241bb5ab5dd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b65b6b622a1441e9984102cf947cfa93",
              "IPY_MODEL_42578936b5ee4854bbea7e5c4d60c3f8",
              "IPY_MODEL_9ac7c1f28bcc46f9aa44cb74588b4b46"
            ],
            "layout": "IPY_MODEL_1dcd182e45c5490f89734699bced780f"
          }
        },
        "b65b6b622a1441e9984102cf947cfa93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca89ee78028446caa31084db2d49a00e",
            "placeholder": "​",
            "style": "IPY_MODEL_19230b69fa144ce6ba8d0623c857d6d0",
            "value": "generation_config.json: 100%"
          }
        },
        "42578936b5ee4854bbea7e5c4d60c3f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5befd6f45c6f440b8a9d1424d705450a",
            "max": 147,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c961c8d112aa498e8f1c0d8755b6cf8c",
            "value": 147
          }
        },
        "9ac7c1f28bcc46f9aa44cb74588b4b46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8541a82ab2a64782ae317d7437b234c4",
            "placeholder": "​",
            "style": "IPY_MODEL_a3c78ccd02804064bcc96e6a9af4b8a8",
            "value": " 147/147 [00:00&lt;00:00, 15.7kB/s]"
          }
        },
        "1dcd182e45c5490f89734699bced780f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca89ee78028446caa31084db2d49a00e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19230b69fa144ce6ba8d0623c857d6d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5befd6f45c6f440b8a9d1424d705450a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c961c8d112aa498e8f1c0d8755b6cf8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8541a82ab2a64782ae317d7437b234c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3c78ccd02804064bcc96e6a9af4b8a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "28855641d47242eabe3d5db238986f08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9d3e39a0bf4340209b19548e642ce47d",
              "IPY_MODEL_aaa6b2777cb34d8998d25d9b6d86bcbf",
              "IPY_MODEL_474948e2c4134aafabd8184a54b7a01d"
            ],
            "layout": "IPY_MODEL_b20c6161fbb848798179afd4040ee627"
          }
        },
        "9d3e39a0bf4340209b19548e642ce47d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0386c95375e641f6b272d94ca91d488e",
            "placeholder": "​",
            "style": "IPY_MODEL_89761d4e2cc740e4a2dc348aebc15a6e",
            "value": "Map: 100%"
          }
        },
        "aaa6b2777cb34d8998d25d9b6d86bcbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a777826a84a942b99f2378a123dfac24",
            "max": 3611,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c7b187b5081f4a01862cea6591690d21",
            "value": 3611
          }
        },
        "474948e2c4134aafabd8184a54b7a01d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_47653497d6c347c5be12b1421960a7e2",
            "placeholder": "​",
            "style": "IPY_MODEL_8e7f036b41364f8ab127481e37816db8",
            "value": " 3611/3611 [00:03&lt;00:00, 813.25 examples/s]"
          }
        },
        "b20c6161fbb848798179afd4040ee627": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0386c95375e641f6b272d94ca91d488e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "89761d4e2cc740e4a2dc348aebc15a6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a777826a84a942b99f2378a123dfac24": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7b187b5081f4a01862cea6591690d21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "47653497d6c347c5be12b1421960a7e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e7f036b41364f8ab127481e37816db8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5c162f7591a547f4a8d79b7a592cf706": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fbab15958cfc47d997f28e27007497e1",
              "IPY_MODEL_75c88558082d4d6081319bee025bffe9",
              "IPY_MODEL_1b17ac15156b43e6adf0f7ed9e3c70f1"
            ],
            "layout": "IPY_MODEL_7fbb55949e0345a69e088d90dc730787"
          }
        },
        "fbab15958cfc47d997f28e27007497e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_14179612262c40c1814169383cc1b2bb",
            "placeholder": "​",
            "style": "IPY_MODEL_9dcc18e1b8a54d378674df750a178659",
            "value": "Map: 100%"
          }
        },
        "75c88558082d4d6081319bee025bffe9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9e12f71a13eb454a90ef3a9f2f78b6ab",
            "max": 3611,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c48450c092ed41f2a6ee55e780cd5d70",
            "value": 3611
          }
        },
        "1b17ac15156b43e6adf0f7ed9e3c70f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6a5a08cab6a4360a729270574fbcc1d",
            "placeholder": "​",
            "style": "IPY_MODEL_659ec6c22d6849debc151e77e0f3423f",
            "value": " 3611/3611 [00:06&lt;00:00, 568.75 examples/s]"
          }
        },
        "7fbb55949e0345a69e088d90dc730787": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "14179612262c40c1814169383cc1b2bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9dcc18e1b8a54d378674df750a178659": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9e12f71a13eb454a90ef3a9f2f78b6ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c48450c092ed41f2a6ee55e780cd5d70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f6a5a08cab6a4360a729270574fbcc1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "659ec6c22d6849debc151e77e0f3423f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8401056cad1441d5837797c80cc9d592": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_07143aed62754b4c89ff4e1dcaa839e7"
          }
        },
        "583b2d093d2f48ddb6fd8d06bf49b12d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9eae44355ce9439780b746d1792dbed1",
            "placeholder": "​",
            "style": "IPY_MODEL_b6fbb81519c94f63b5c138891e7448f2",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "8459050797154af480d5e23ae87a2315": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_a076eadd9ddf4256a87c70b605b0b64d",
            "placeholder": "​",
            "style": "IPY_MODEL_45593f989d03451a86da011149a30256",
            "value": ""
          }
        },
        "ac8a66fb9a7a499189567be3ce7eaaad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_23355aa09c134cfd95ba6b608288e49c",
            "style": "IPY_MODEL_644fa38df99042cd9660cb7ae1cf55c5",
            "value": true
          }
        },
        "4715632c00874939af6ba9f7985634e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_48ef4e732f654aafbc76dedaf6e1ed2c",
            "style": "IPY_MODEL_59bd48674bfc4eeca010117e0953b8d1",
            "tooltip": ""
          }
        },
        "1567ddc4a1e94b318be587b7bae44a30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_88dae507de5e4d98a8c71a96d8ad9e0b",
            "placeholder": "​",
            "style": "IPY_MODEL_1f3c546bf0cd4c2fbdba09e221f42ad8",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "07143aed62754b4c89ff4e1dcaa839e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "9eae44355ce9439780b746d1792dbed1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6fbb81519c94f63b5c138891e7448f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a076eadd9ddf4256a87c70b605b0b64d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45593f989d03451a86da011149a30256": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "23355aa09c134cfd95ba6b608288e49c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "644fa38df99042cd9660cb7ae1cf55c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "48ef4e732f654aafbc76dedaf6e1ed2c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59bd48674bfc4eeca010117e0953b8d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "88dae507de5e4d98a8c71a96d8ad9e0b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f3c546bf0cd4c2fbdba09e221f42ad8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "520403307c6249ffa62fcfe12daf6e79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_adebeaccdc324e609b6b6541af390928",
            "placeholder": "​",
            "style": "IPY_MODEL_4b3c081da19e4a0186f3e60883c7de78",
            "value": "Connecting..."
          }
        },
        "adebeaccdc324e609b6b6541af390928": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b3c081da19e4a0186f3e60883c7de78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Depenedencies"
      ],
      "metadata": {
        "id": "ucWm1mJOP9n1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "gdTW7eYlkbmI",
        "outputId": "c24bca20-eea7-445c-f5af-b64feab98253"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.5.1-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.15.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft) (1.6.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading datasets-3.5.1-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.4/491.4 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m120.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fsspec, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, nvidia-cusolver-cu12, datasets, evaluate\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.1 dill-0.3.8 evaluate-0.4.3 fsspec-2025.3.0 multiprocess-0.70.16 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers datasets torch pandas evaluate peft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "outputId": "bd991126-8422-4036-b38a-6d14f3a556f7",
        "id": "4Gcf96vDQQFi",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.7.4.2)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.4.26)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.4.1)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle) (5.29.4)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.4.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle) (0.5.1)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-af3a2a95-12b8-48bb-8999-cc4f276fc476\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-af3a2a95-12b8-48bb-8999-cc4f276fc476\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "Dataset URL: https://www.kaggle.com/datasets/mazenmahmoud79/txttosql-nlp\n",
            "License(s): unknown\n",
            "AR_spider.jsonl  database\n"
          ]
        }
      ],
      "source": [
        "!pip install kaggle\n",
        "from google.colab import files\n",
        "\n",
        "# Upload Kaggle API key (you only need to do this once)\n",
        "files.upload()\n",
        "\n",
        "\n",
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "\n",
        "!kaggle datasets download -d mazenmahmoud79/txttosql-nlp\n",
        "\n",
        "\n",
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile(\"txttosql-nlp.zip\", \"r\") as zip_ref:\n",
        "    zip_ref.extractall(\"data\")  # Extract files into 'data' folder\n",
        "\n",
        "# List extracted files\n",
        "!ls data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJLNj-lwQQGL"
      },
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "!pip install -q sqlparse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkik7kR5UeIP",
        "outputId": "92e8ded9-f68a-416c-86fb-720d6b08aaf3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "8401056cad1441d5837797c80cc9d592",
            "583b2d093d2f48ddb6fd8d06bf49b12d",
            "8459050797154af480d5e23ae87a2315",
            "ac8a66fb9a7a499189567be3ce7eaaad",
            "4715632c00874939af6ba9f7985634e8",
            "1567ddc4a1e94b318be587b7bae44a30",
            "07143aed62754b4c89ff4e1dcaa839e7",
            "9eae44355ce9439780b746d1792dbed1",
            "b6fbb81519c94f63b5c138891e7448f2",
            "a076eadd9ddf4256a87c70b605b0b64d",
            "45593f989d03451a86da011149a30256",
            "23355aa09c134cfd95ba6b608288e49c",
            "644fa38df99042cd9660cb7ae1cf55c5",
            "48ef4e732f654aafbc76dedaf6e1ed2c",
            "59bd48674bfc4eeca010117e0953b8d1",
            "88dae507de5e4d98a8c71a96d8ad9e0b",
            "1f3c546bf0cd4c2fbdba09e221f42ad8",
            "520403307c6249ffa62fcfe12daf6e79",
            "adebeaccdc324e609b6b6541af390928",
            "4b3c081da19e4a0186f3e60883c7de78"
          ]
        },
        "outputId": "6a4f9676-a122-4627-d413-4b80cf9ff6d7",
        "id": "749zNL3OQh1i"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8401056cad1441d5837797c80cc9d592"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Authenticate for LLaMA access\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()  # Paste your HuggingFace token when prompted\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b8bb247-1d73-404c-cc0a-a2da5c87fd13",
        "id": "xNyaYyz6Qh1j"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/126.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.4/126.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q pyarabic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "63386e5d-3302-4264-ba07-fcaf9e422371",
        "id": "9TzbuojyQh1k"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n",
            "Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.45.5\n"
          ]
        }
      ],
      "source": [
        "!pip install -U bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q rouge_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5Op8-pfTGuo",
        "outputId": "0ce71e7b-1292-4a1e-eb66-ab11dc9e642b",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-sal8nuy3Et"
      },
      "source": [
        "# Llama Embedding and Tokenization using BERT - JSONL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "99a06b90503443baa6d2efc4e37aabf6",
            "f3804c22e2874bffb5ac872703c76828"
          ]
        },
        "id": "pOCr1s8tr-WR",
        "outputId": "6dd69e7a-8f5d-4650-de56-bb49c7c63939"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "99a06b90503443baa6d2efc4e37aabf6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Authenticate for LLaMA access\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()  # Paste your HuggingFace token when prompted\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPBhUOqQrG72",
        "outputId": "4b8bb247-1d73-404c-cc0a-a2da5c87fd13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/126.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.4/126.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q pyarabic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "DSTq0inNzKOI",
        "outputId": "cadc9075-71b8-4beb-9460-885cc05b79b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n",
            "Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m88.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m78.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bitsandbytes\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed bitsandbytes-0.45.5 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "9148acda18f74a2e8111395772516741",
              "pip_warning": {
                "packages": [
                  "nvidia"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install -U bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epJJm8HWGLew",
        "outputId": "25a014eb-232d-4aa3-e21d-015761af1951"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Loaded CAMeL-BERT model\n",
            "✅ Loaded Llama-2-7b tokenizer\n",
            "📄 Input file: /content/cleaned_AR_spider.jsonl\n",
            "Total lines: 4525\n",
            "\n",
            "🔍 Raw content of first 2 lines:\n",
            "Line 0: {\"question\": \"How many heads of the departments are older than 56 ?\", \"query\": \"SELECT count(*) FROM...\n",
            "Line 1: {\"question\": \"List the name, born state and age of the heads of departments ordered by age.\", \"query...\n",
            "\n",
            "🔍 Parsed content of first 2 lines:\n",
            "Line 0: {'question': 'How many heads of the departments are older than 56 ?', 'query': 'SELECT count(*) FROM head WHERE age  >  56', 'arabic': 'كم عدد رؤساء الأقسام الذين تزيد أعمارهم عن 56 سنة؟', 'db_id': 'department_management'}\n",
            "Line 1: {'question': 'List the name, born state and age of the heads of departments ordered by age.', 'query': 'SELECT name ,  born_state ,  age FROM head ORDER BY age', 'arabic': 'اعرض قائمة بأسماء رؤساء الأقسام، مكان ميلادهم، وأعمارهم مرتبة حسب العمر.', 'db_id': 'department_management'}\n",
            "\n",
            "📝 Starting file processing...\n",
            "✅ Processed 0 lines...\n",
            "✅ Processed 100 lines...\n",
            "✅ Processed 200 lines...\n",
            "✅ Processed 300 lines...\n",
            "Skipped line 366: Text not primarily Arabic - [Question] كم عدد الطلاب؟...\n",
            "Skipped line 393: Text not primarily Arabic - [Question] ما هو أكبر تخصص؟...\n",
            "✅ Processed 400 lines...\n",
            "✅ Processed 500 lines...\n",
            "✅ Processed 600 lines...\n",
            "Skipped line 676: Text not primarily Arabic - [Question] ما هو الحد الأقصى لقيمة OMIM (Online Me...\n",
            "Skipped line 692: Text not primarily Arabic - [Question] ما هو نوع التفاعل بين الإنزيم المسمى 'A...\n",
            "Skipped line 693: Text not primarily Arabic - [Question] ما هو نوع التفاعل للإنزيم المسمى 'ALA s...\n",
            "✅ Processed 700 lines...\n",
            "Skipped line 775: Text not primarily Arabic - [Question] عدد البلدان....\n",
            "✅ Processed 800 lines...\n",
            "✅ Processed 900 lines...\n",
            "✅ Processed 1000 lines...\n",
            "✅ Processed 1100 lines...\n",
            "✅ Processed 1200 lines...\n",
            "✅ Processed 1300 lines...\n",
            "✅ Processed 1400 lines...\n",
            "Skipped line 1451: Text not primarily Arabic - متى أصبحت التسريع 'CACHEbox' والمتصفح 'Internet Ex...\n",
            "✅ Processed 1500 lines...\n",
            "✅ Processed 1600 lines...\n",
            "✅ Processed 1700 lines...\n",
            "✅ Processed 1800 lines...\n",
            "✅ Processed 1900 lines...\n",
            "✅ Processed 2000 lines...\n",
            "✅ Processed 2100 lines...\n",
            "✅ Processed 2200 lines...\n",
            "✅ Processed 2300 lines...\n",
            "✅ Processed 2400 lines...\n",
            "✅ Processed 2500 lines...\n",
            "✅ Processed 2600 lines...\n",
            "✅ Processed 2700 lines...\n",
            "✅ Processed 2800 lines...\n",
            "Skipped line 2888: Text not primarily Arabic - كم كان عدد اللاعبين في فريق Boston Red Stockings ف...\n",
            "✅ Processed 2900 lines...\n",
            "✅ Processed 3000 lines...\n",
            "✅ Processed 3100 lines...\n",
            "✅ Processed 3200 lines...\n",
            "Skipped line 3252: Text not primarily Arabic - ما هي تواريخ بداية المنح التي لها وصف 'Regular' و ...\n",
            "Skipped line 3278: Text not primarily Arabic - ما هي الأدوار التي قام بها أعضاء الفريق بين '2003-...\n",
            "✅ Processed 3300 lines...\n",
            "✅ Processed 3400 lines...\n",
            "✅ Processed 3500 lines...\n",
            "✅ Processed 3600 lines...\n",
            "✅ Processed 3700 lines...\n",
            "✅ Processed 3800 lines...\n",
            "✅ Processed 3900 lines...\n",
            "✅ Processed 4000 lines...\n",
            "✅ Processed 4100 lines...\n",
            "✅ Processed 4200 lines...\n",
            "✅ Processed 4300 lines...\n",
            "✅ Processed 4400 lines...\n",
            "Skipped line 4458: Text not primarily Arabic - ما هي جميع العناوين في East Julianaside، Texas أو ...\n",
            "✅ Processed 4500 lines...\n",
            "💾 Embeddings saved to /content/drive/MyDrive/llama_processed/arabic_embeddings.pt\n",
            "\n",
            "📊 Summary\n",
            "Total lines: 4525\n",
            "Valid lines: 4514\n",
            "Skipped lines: 11\n",
            "💾 Metadata saved to /content/drive/MyDrive/llama_processed/tokenized_arabic.jsonl\n",
            "💾 Llama tokens saved to /content/drive/MyDrive/llama_processed/llama_tokenized.jsonl\n",
            "💾 Invalid lines saved to /content/drive/MyDrive/llama_processed/invalid_lines.jsonl\n",
            "\n",
            "🔍 Validating output...\n",
            "Total metadata entries: 4514\n",
            "{\"id\": \"b6495215-cfdc-4505-9219-3f17d912934a\", \"original_text\": \"كم عدد رؤساء الأقسام الذين تزيد أعمارهم عن 56 سنة؟\", \"normalized_text\": \"كم عدد رءساء الءقسام الذين تزيد ءعمارهم عن 56 سنة؟\", \"db_id\": \"department_management\", \"question\": \"How many heads of the departments are older than 56 ?\"}\n",
            "{\"id\": \"2fc2c393-168a-407d-a59e-03c1df171ee3\", \"original_text\": \"اعرض قائمة بأسماء رؤساء الأقسام، مكان ميلادهم، وأعمارهم مرتبة حسب العمر.\", \"normalized_text\": \"اعرض قاءمة بءسماء رءساء الءقسام، مكان ميلادهم، وءعمارهم مرتبة حسب العمر.\", \"db_id\": \"department_management\", \"question\": \"List the name, born state and age of the heads of departments ordered by age.\"}\n",
            "Total embeddings: 4514\n",
            "Embedding shape: torch.Size([4514, 768])\n",
            "Total Llama tokenized entries: 4514\n",
            "{\"id\": \"2b406516-12f9-47a4-8ad7-d39f915c67ec\", \"original_text\": \"كم عدد رؤساء الأقسام الذين تزيد أعمارهم عن 56 سنة؟\", \"normalized_text\": \"كم عدد رءساء الءقسام الذين تزيد ءعمارهم عن 56 سنة؟\", \"token_ids\": [1, 29871, 30283, 30159, 29871, 30218, 30172, 30172, 29871, 30156, 30992, 30198, 30112, 30992, 24508, 30992, 30265, 30198, 30112, 30159, 24508, 30851, 30163, 30162, 29871, 30195, 30295, 30163, 30172, 29871, 30992, 30218, 30159, 30112, 30156, 30204, 30159, 29871, 30218, 30162, 29871, 29945, 29953, 29871, 30198, 30162, 30242, 219, 162], \"db_id\": \"department_management\", \"question\": \"How many heads of the departments are older than 56 ?\"}\n",
            "{\"id\": \"2970f0cb-68b9-4437-acd6-4ed38c105f79\", \"original_text\": \"اعرض قائمة بأسماء رؤساء الأقسام، مكان ميلادهم، وأعمارهم مرتبة حسب العمر.\", \"normalized_text\": \"اعرض قاءمة بءسماء رءساء الءقسام، مكان ميلادهم، وءعمارهم مرتبة حسب العمر.\", \"token_ids\": [1, 29871, 30112, 30218, 30156, 30624, 29871, 30265, 30112, 30992, 30159, 30242, 29871, 30177, 30992, 30198, 30159, 30112, 30992, 29871, 30156, 30992, 30198, 30112, 30992, 24508, 30992, 30265, 30198, 30112, 30159, 31116, 29871, 30159, 30283, 30112, 30162, 29871, 30159, 30163, 30138, 30112, 30172, 30204, 30159, 31116, 29871, 30171, 30992, 30218, 30159, 30112, 30156, 30204, 30159, 29871, 30159, 30156, 30195, 30177, 30242, 29871, 30240, 30198, 30177, 24508, 30218, 30159, 30156, 29889], \"db_id\": \"department_management\", \"question\": \"List the name, born state and age of the heads of departments ordered by age.\"}\n",
            "Total invalid lines: 11\n",
            "Invalid line sample: {\"line_num\": 366, \"reason\": \"Text not primarily Arabic\", \"content\": \"{\\\"question\\\": \\\"How many students are there?\\\", \\\"query\\\": \\\"SELECT count(*) FROM Student\\\", \\\"arabic\\\": \\\"[Question] كم عدد الطلاب؟\\\", \\\"db_id\\\": \\\"allergy_1\\\"}\\n\"}\n",
            "Invalid line sample: {\"line_num\": 393, \"reason\": \"Text not primarily Arabic\", \"content\": \"{\\\"question\\\": \\\"What is the largest major?\\\", \\\"query\\\": \\\"SELECT major FROM Student GROUP BY major ORDER BY count(*) DESC LIMIT 1\\\", \\\"arabic\\\": \\\"[Question] ما هو أكبر تخصص؟\\\", \\\"db_id\\\": \\\"allergy_1\\\"}\\n\"}\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import re\n",
        "import torch\n",
        "import os\n",
        "from pyarabic import araby\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from huggingface_hub import login\n",
        "import uuid\n",
        "import unicodedata\n",
        "\n",
        "# Step 1: Set Device & Authenticate\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "hf_token = \"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\"\n",
        "login(token=hf_token)\n",
        "\n",
        "# Step 2: Define Paths\n",
        "input_path = \"/content/cleaned_AR_spider.jsonl\"\n",
        "output_dir = \"/content/drive/MyDrive/llama_processed/\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "cleaned_output_path = f\"{output_dir}cleaned_AR_spider.jsonl\"\n",
        "embeddings_output_path = f\"{output_dir}arabic_embeddings.pt\"\n",
        "metadata_output_path = f\"{output_dir}tokenized_arabic.jsonl\"\n",
        "llama_tokens_output_path = f\"{output_dir}llama_tokenized.jsonl\"\n",
        "invalid_lines_path = f\"{output_dir}invalid_lines.jsonl\"\n",
        "\n",
        "# Step 3: Load Models\n",
        "def load_camel_model():\n",
        "    \"\"\"Load CAMeL-BERT model in 8-bit to save memory\"\"\"\n",
        "    from transformers import BitsAndBytesConfig\n",
        "    bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\"CAMeL-Lab/bert-base-arabic-camelbert-mix\", use_fast=False)\n",
        "        model = AutoModel.from_pretrained(\n",
        "            \"CAMeL-Lab/bert-base-arabic-camelbert-mix\",\n",
        "            quantization_config=bnb_config,\n",
        "            device_map=\"auto\"\n",
        "        ).eval()\n",
        "        print(\"✅ Loaded CAMeL-BERT model\")\n",
        "        return tokenizer, model\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Failed to load CAMeL-BERT: {str(e)}\")\n",
        "        return None, None\n",
        "\n",
        "def load_llama_tokenizer():\n",
        "    \"\"\"Load Llama-2-7b tokenizer with authentication\"\"\"\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\n",
        "            \"meta-llama/Llama-2-7b\",\n",
        "            token=hf_token,\n",
        "            legacy=True\n",
        "        )\n",
        "        print(\"✅ Loaded Llama-2-7b tokenizer\")\n",
        "        return tokenizer\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Failed to load Llama-2-7b tokenizer: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Load models\n",
        "camel_tokenizer, camel_model = load_camel_model()\n",
        "llama_tokenizer = load_llama_tokenizer()\n",
        "if camel_tokenizer is None or camel_model is None or llama_tokenizer is None:\n",
        "    raise Exception(\"Failed to load models. Exiting.\")\n",
        "\n",
        "# Step 4: Normalize Arabic Text\n",
        "def normalize_arabic(text):\n",
        "    \"\"\"Normalize Arabic text by removing diacritics and normalizing characters\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    # Remove diacritics\n",
        "    text = araby.strip_tashkeel(text)\n",
        "    # Normalize hamza to standard forms (e.g., رؤساء instead of رءساء)\n",
        "    text = araby.normalize_hamza(text, method=\"all_to_madda\")\n",
        "    # Normalize alef variants\n",
        "    text = araby.normalize_alef(text)\n",
        "    # Remove tatweel and normalize ligatures\n",
        "    text = araby.strip_tatweel(text)\n",
        "    text = araby.normalize_ligature(text)\n",
        "    # Ensure correct Unicode normalization\n",
        "    text = unicodedata.normalize(\"NFC\", text)\n",
        "    return text.strip()\n",
        "\n",
        "# Step 5: Clean and Validate JSON Lines\n",
        "def clean_json_line(line):\n",
        "    \"\"\"Clean JSON line to ensure valid format\"\"\"\n",
        "    line = line.strip().replace('\\ufeff', '')\n",
        "    line = re.sub(r'(\\w+)(?=\\s*:\\s*[\"{])', r'\"\\1\"', line)\n",
        "    line = line.replace('\\\\\"', '\"').replace(\"\\\\'\", \"'\")\n",
        "    return line\n",
        "\n",
        "def validate_json(line):\n",
        "    \"\"\"Validate and parse JSON line, always return (data, error)\"\"\"\n",
        "    try:\n",
        "        data = json.loads(line)\n",
        "        return data, None\n",
        "    except json.JSONDecodeError as e:\n",
        "        try:\n",
        "            fixed = re.sub(r'([{,])(\\w+?)\\s*:', r'\\1\"\\2\":', line)\n",
        "            data = json.loads(fixed)\n",
        "            return data, None\n",
        "        except json.JSONDecodeError:\n",
        "            return None, f\"Invalid JSON: {str(e)}\"\n",
        "\n",
        "# Step 6: Validate Input File\n",
        "def validate_input_file():\n",
        "    \"\"\"Check if input file exists and is valid\"\"\"\n",
        "    if not os.path.exists(input_path):\n",
        "        raise FileNotFoundError(f\"Input file not found: {input_path}\")\n",
        "\n",
        "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        lines = f.readlines()\n",
        "        if not lines:\n",
        "            raise ValueError(f\"Input file is empty: {input_path}\")\n",
        "\n",
        "        print(f\"📄 Input file: {input_path}\")\n",
        "        print(f\"Total lines: {len(lines)}\")\n",
        "        print(\"\\n🔍 Raw content of first 2 lines:\")\n",
        "        for i, line in enumerate(lines[:2]):\n",
        "            print(f\"Line {i}: {line.strip()[:100]}...\")\n",
        "\n",
        "        print(\"\\n🔍 Parsed content of first 2 lines:\")\n",
        "        for i, line in enumerate(lines[:2]):\n",
        "            cleaned_line = clean_json_line(line)\n",
        "            data, error = validate_json(cleaned_line)\n",
        "            if error:\n",
        "                print(f\"Line {i}: {error}\")\n",
        "            else:\n",
        "                print(f\"Line {i}: {data}\")\n",
        "\n",
        "        return len(lines)\n",
        "\n",
        "# Step 7: Process File\n",
        "def process_file():\n",
        "    \"\"\"Process JSONL file, generate embeddings, and save results\"\"\"\n",
        "    # Validate input file\n",
        "    total_lines = validate_input_file()\n",
        "\n",
        "    all_embeddings = []\n",
        "    metadata = []\n",
        "    llama_tokens = []\n",
        "    line_count = 0\n",
        "    skipped_lines = 0\n",
        "\n",
        "    print(\"\\n📝 Starting file processing...\")\n",
        "    with open(input_path, \"r\", encoding=\"utf-8\") as infile, \\\n",
        "         open(cleaned_output_path, \"w\", encoding=\"utf-8\") as outfile, \\\n",
        "         open(metadata_output_path, \"w\", encoding=\"utf-8\") as meta_file, \\\n",
        "         open(llama_tokens_output_path, \"w\", encoding=\"utf-8\") as llama_file, \\\n",
        "         open(invalid_lines_path, \"w\", encoding=\"utf-8\") as invalid_file:\n",
        "\n",
        "        for line_num, line in enumerate(infile):\n",
        "            line_count += 1\n",
        "            try:\n",
        "                # Clean JSON line\n",
        "                cleaned_line = clean_json_line(line)\n",
        "                if not cleaned_line:\n",
        "                    skipped_lines += 1\n",
        "                    print(f\"Skipped line {line_num}: Empty line\")\n",
        "                    json.dump({\"line_num\": line_num, \"reason\": \"Empty line\", \"content\": line}, invalid_file, ensure_ascii=False)\n",
        "                    invalid_file.write(\"\\n\")\n",
        "                    continue\n",
        "\n",
        "                # Validate JSON\n",
        "                data, error = validate_json(cleaned_line)\n",
        "                if error:\n",
        "                    skipped_lines += 1\n",
        "                    print(f\"Skipped line {line_num}: {error}\")\n",
        "                    json.dump({\"line_num\": line_num, \"reason\": error, \"content\": line}, invalid_file, ensure_ascii=False)\n",
        "                    invalid_file.write(\"\\n\")\n",
        "                    continue\n",
        "\n",
        "                # Get Arabic text (prefer 'arabic', fall back to 'question')\n",
        "                arabic_text = data.get(\"arabic\", data.get(\"question\", \"\"))\n",
        "                if not arabic_text:\n",
        "                    skipped_lines += 1\n",
        "                    print(f\"Skipped line {line_num}: No Arabic text (missing 'arabic' or 'question')\")\n",
        "                    json.dump({\"line_num\": line_num, \"reason\": \"No Arabic text\", \"content\": line}, invalid_file, ensure_ascii=False)\n",
        "                    invalid_file.write(\"\\n\")\n",
        "                    continue\n",
        "\n",
        "                # Check if text is likely Arabic\n",
        "                arabic_chars = sum(1 for c in arabic_text if 0x0600 <= ord(c) <= 0x06FF)\n",
        "                if arabic_chars / len(arabic_text) < 0.5:\n",
        "                    skipped_lines += 1\n",
        "                    print(f\"Skipped line {line_num}: Text not primarily Arabic - {arabic_text[:50]}...\")\n",
        "                    json.dump({\"line_num\": line_num, \"reason\": \"Text not primarily Arabic\", \"content\": line}, invalid_file, ensure_ascii=False)\n",
        "                    invalid_file.write(\"\\n\")\n",
        "                    continue\n",
        "\n",
        "                # Normalize Arabic text\n",
        "                normalized_text = normalize_arabic(arabic_text)\n",
        "\n",
        "                # Tokenize with Llama-2 for training\n",
        "                try:\n",
        "                    llama_token_ids = llama_tokenizer.encode(\n",
        "                        normalized_text,\n",
        "                        max_length=512,\n",
        "                        truncation=True\n",
        "                    )\n",
        "                except Exception as e:\n",
        "                    skipped_lines += 1\n",
        "                    print(f\"Skipped line {line_num}: Llama tokenization error - {str(e)}\")\n",
        "                    json.dump({\"line_num\": line_num, \"reason\": f\"Llama tokenization error: {str(e)}\", \"content\": line}, invalid_file, ensure_ascii=False)\n",
        "                    invalid_file.write(\"\\n\")\n",
        "                    continue\n",
        "\n",
        "                llama_token_entry = {\n",
        "                    \"id\": str(uuid.uuid4()),\n",
        "                    \"original_text\": arabic_text[:1000],\n",
        "                    \"normalized_text\": normalized_text[:1000],\n",
        "                    \"token_ids\": llama_token_ids,\n",
        "                    \"db_id\": data.get(\"db_id\", \"\"),\n",
        "                    \"question\": data.get(\"question\", \"\")\n",
        "                }\n",
        "                llama_tokens.append(llama_token_entry)\n",
        "                json.dump(llama_token_entry, llama_file, ensure_ascii=False)\n",
        "                llama_file.write(\"\\n\")\n",
        "\n",
        "                # Tokenize and generate embedding with CAMeL-BERT\n",
        "                try:\n",
        "                    camel_tokens = camel_tokenizer(\n",
        "                        normalized_text,\n",
        "                        max_length=512,\n",
        "                        padding=\"max_length\",\n",
        "                        truncation=True,\n",
        "                        return_tensors=\"pt\"\n",
        "                    ).to(device)\n",
        "\n",
        "                    with torch.no_grad():\n",
        "                        outputs = camel_model(**camel_tokens)\n",
        "                        cls_embedding = outputs.last_hidden_state[:, 0, :].cpu().half()\n",
        "                except Exception as e:\n",
        "                    skipped_lines += 1\n",
        "                    print(f\"Skipped line {line_num}: CAMeL-BERT processing error - {str(e)}\")\n",
        "                    json.dump({\"line_num\": line_num, \"reason\": f\"CAMeL-BERT processing error: {str(e)}\", \"content\": line}, invalid_file, ensure_ascii=False)\n",
        "                    invalid_file.write(\"\\n\")\n",
        "                    continue\n",
        "\n",
        "                # Store results\n",
        "                all_embeddings.append(cls_embedding)\n",
        "                meta_entry = {\n",
        "                    \"id\": str(uuid.uuid4()),\n",
        "                    \"original_text\": arabic_text[:1000],\n",
        "                    \"normalized_text\": normalized_text[:1000],\n",
        "                    \"db_id\": data.get(\"db_id\", \"\"),\n",
        "                    \"question\": data.get(\"question\", \"\")\n",
        "                }\n",
        "                metadata.append(meta_entry)\n",
        "\n",
        "                # Write cleaned JSON and metadata\n",
        "                json.dump(data, outfile, ensure_ascii=False)\n",
        "                outfile.write(\"\\n\")\n",
        "                json.dump(meta_entry, meta_file, ensure_ascii=False)\n",
        "                meta_file.write(\"\\n\")\n",
        "\n",
        "                if line_num % 100 == 0:\n",
        "                    print(f\"✅ Processed {line_num} lines...\")\n",
        "\n",
        "            except Exception as e:\n",
        "                skipped_lines += 1\n",
        "                print(f\"❌ Unexpected error at line {line_num}: {str(e)}\")\n",
        "                json.dump({\"line_num\": line_num, \"reason\": f\"Unexpected error: {str(e)}\", \"content\": line}, invalid_file, ensure_ascii=False)\n",
        "                invalid_file.write(\"\\n\")\n",
        "                continue\n",
        "\n",
        "        # Save embeddings if any were generated\n",
        "        if all_embeddings:\n",
        "            torch.save({\n",
        "                \"embeddings\": torch.cat(all_embeddings, dim=0),\n",
        "                \"metadata\": metadata\n",
        "            }, embeddings_output_path)\n",
        "            print(f\"💾 Embeddings saved to {embeddings_output_path}\")\n",
        "        else:\n",
        "            print(\"⚠️ No embeddings generated. Check input data or invalid_lines.jsonl for issues.\")\n",
        "\n",
        "        print(f\"\\n📊 Summary\\nTotal lines: {line_count}\\nValid lines: {len(all_embeddings)}\\nSkipped lines: {skipped_lines}\")\n",
        "        print(f\"💾 Metadata saved to {metadata_output_path}\")\n",
        "        print(f\"💾 Llama tokens saved to {llama_tokens_output_path}\")\n",
        "        print(f\"💾 Invalid lines saved to {invalid_lines_path}\")\n",
        "\n",
        "# Step 8: Run the Pipeline\n",
        "try:\n",
        "    process_file()\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"❌ Error: {str(e)}\")\n",
        "except ValueError as e:\n",
        "    print(f\"❌ Error: {str(e)}\")\n",
        "\n",
        "# Step 9: Validate Output\n",
        "print(\"\\n🔍 Validating output...\")\n",
        "try:\n",
        "    with open(metadata_output_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        lines = f.readlines()\n",
        "        print(f\"Total metadata entries: {len(lines)}\")\n",
        "        for i in range(min(2, len(lines))):\n",
        "            print(lines[i].strip())\n",
        "\n",
        "    if os.path.exists(embeddings_output_path):\n",
        "        data = torch.load(embeddings_output_path, weights_only=False)\n",
        "        print(\"Total embeddings:\", len(data[\"metadata\"]))\n",
        "        print(\"Embedding shape:\", data[\"embeddings\"].shape)\n",
        "    else:\n",
        "        print(\"⚠️ Embeddings file not found.\")\n",
        "\n",
        "    with open(llama_tokens_output_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        llama_lines = f.readlines()\n",
        "        print(\"Total Llama tokenized entries:\", len(llama_lines))\n",
        "        for i in range(min(2, len(llama_lines))):\n",
        "            print(llama_lines[i].strip())\n",
        "\n",
        "    if os.path.exists(invalid_lines_path):\n",
        "        with open(invalid_lines_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            invalid_lines = f.readlines()\n",
        "            print(f\"Total invalid lines: {len(invalid_lines)}\")\n",
        "            for i in range(min(2, len(invalid_lines))):\n",
        "                print(f\"Invalid line sample: {invalid_lines[i].strip()}\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Validation failed: {str(e)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26Gn-fOcHw7M",
        "outputId": "2483783b-a76d-4b81-9553-adc3abe46c0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1, 29871, 30283, 30159, 29871, 30218, 30172, 30172, 29871, 30156, 219, 167, 30198, 30112, 30992, 24508, 30372, 30265, 30198, 30112, 30159, 219, 162]\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b\", token=\"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\", legacy=True)\n",
        "print(tokenizer.encode(\"كم عدد رؤساء الأقسام؟\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121,
          "referenced_widgets": [
            "b8e452eccc814bd89628cfedeefca737",
            "5881a4751f8e4dd285a21e93b0e93897",
            "aba02276fbd1429f95d0238acbe9faf1",
            "e4b59629b61442c1be9868a1d2519fd5",
            "88f74ac9bc7b4958ab71eb4ba896ba27",
            "5933656a621845b38eb07c5e46fd5e67",
            "d2056be7d8a74bf5a71df0bffa4bce85",
            "b3f03442a844440f9d56d2f518127ef2",
            "2b36cbbbfd174fd9939a2770c7b90101",
            "9e4d3688e498498499c0fa72fc6b5565",
            "fd2a7f197c59477baf537c4d6ee40356"
          ]
        },
        "id": "T-o7GKJqHJ0y",
        "outputId": "910ac393-e7a2-41fb-90cd-4692879bb922"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b8e452eccc814bd89628cfedeefca737",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1, 4321]\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b\", token=\"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\")\n",
        "print(tokenizer.encode(\"Test\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKqDdD6kziAv",
        "outputId": "fdc925a0-1fd2-4227-d290-23a0d8487372"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🔍 Validating output...\n",
            "Total metadata entries: 4514\n",
            "{\"id\": \"b6495215-cfdc-4505-9219-3f17d912934a\", \"original_text\": \"كم عدد رؤساء الأقسام الذين تزيد أعمارهم عن 56 سنة؟\", \"normalized_text\": \"كم عدد رءساء الءقسام الذين تزيد ءعمارهم عن 56 سنة؟\", \"db_id\": \"department_management\", \"question\": \"How many heads of the departments are older than 56 ?\"}\n",
            "{\"id\": \"2fc2c393-168a-407d-a59e-03c1df171ee3\", \"original_text\": \"اعرض قائمة بأسماء رؤساء الأقسام، مكان ميلادهم، وأعمارهم مرتبة حسب العمر.\", \"normalized_text\": \"اعرض قاءمة بءسماء رءساء الءقسام، مكان ميلادهم، وءعمارهم مرتبة حسب العمر.\", \"db_id\": \"department_management\", \"question\": \"List the name, born state and age of the heads of departments ordered by age.\"}\n",
            "Total embeddings: 4514\n",
            "Embedding shape: torch.Size([4514, 768])\n",
            "Total Llama tokenized entries: 4514\n",
            "{\"id\": \"2b406516-12f9-47a4-8ad7-d39f915c67ec\", \"original_text\": \"كم عدد رؤساء الأقسام الذين تزيد أعمارهم عن 56 سنة؟\", \"normalized_text\": \"كم عدد رءساء الءقسام الذين تزيد ءعمارهم عن 56 سنة؟\", \"token_ids\": [1, 29871, 30283, 30159, 29871, 30218, 30172, 30172, 29871, 30156, 30992, 30198, 30112, 30992, 24508, 30992, 30265, 30198, 30112, 30159, 24508, 30851, 30163, 30162, 29871, 30195, 30295, 30163, 30172, 29871, 30992, 30218, 30159, 30112, 30156, 30204, 30159, 29871, 30218, 30162, 29871, 29945, 29953, 29871, 30198, 30162, 30242, 219, 162], \"db_id\": \"department_management\", \"question\": \"How many heads of the departments are older than 56 ?\"}\n",
            "{\"id\": \"2970f0cb-68b9-4437-acd6-4ed38c105f79\", \"original_text\": \"اعرض قائمة بأسماء رؤساء الأقسام، مكان ميلادهم، وأعمارهم مرتبة حسب العمر.\", \"normalized_text\": \"اعرض قاءمة بءسماء رءساء الءقسام، مكان ميلادهم، وءعمارهم مرتبة حسب العمر.\", \"token_ids\": [1, 29871, 30112, 30218, 30156, 30624, 29871, 30265, 30112, 30992, 30159, 30242, 29871, 30177, 30992, 30198, 30159, 30112, 30992, 29871, 30156, 30992, 30198, 30112, 30992, 24508, 30992, 30265, 30198, 30112, 30159, 31116, 29871, 30159, 30283, 30112, 30162, 29871, 30159, 30163, 30138, 30112, 30172, 30204, 30159, 31116, 29871, 30171, 30992, 30218, 30159, 30112, 30156, 30204, 30159, 29871, 30159, 30156, 30195, 30177, 30242, 29871, 30240, 30198, 30177, 24508, 30218, 30159, 30156, 29889], \"db_id\": \"department_management\", \"question\": \"List the name, born state and age of the heads of departments ordered by age.\"}\n",
            "Total invalid lines: 11\n",
            "Invalid line sample: {\"line_num\": 366, \"reason\": \"Text not primarily Arabic\", \"content\": \"{\\\"question\\\": \\\"How many students are there?\\\", \\\"query\\\": \\\"SELECT count(*) FROM Student\\\", \\\"arabic\\\": \\\"[Question] كم عدد الطلاب؟\\\", \\\"db_id\\\": \\\"allergy_1\\\"}\\n\"}\n",
            "Invalid line sample: {\"line_num\": 393, \"reason\": \"Text not primarily Arabic\", \"content\": \"{\\\"question\\\": \\\"What is the largest major?\\\", \\\"query\\\": \\\"SELECT major FROM Student GROUP BY major ORDER BY count(*) DESC LIMIT 1\\\", \\\"arabic\\\": \\\"[Question] ما هو أكبر تخصص؟\\\", \\\"db_id\\\": \\\"allergy_1\\\"}\\n\"}\n"
          ]
        }
      ],
      "source": [
        "# Step 9: Validate Output\n",
        "print(\"\\n🔍 Validating output...\")\n",
        "try:\n",
        "    with open(metadata_output_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        lines = f.readlines()\n",
        "        print(f\"Total metadata entries: {len(lines)}\")\n",
        "        for i in range(min(2, len(lines))):\n",
        "            print(lines[i].strip())\n",
        "\n",
        "    if os.path.exists(embeddings_output_path):\n",
        "        data = torch.load(embeddings_output_path, weights_only=False)\n",
        "        print(\"Total embeddings:\", len(data[\"metadata\"]))\n",
        "        print(\"Embedding shape:\", data[\"embeddings\"].shape)\n",
        "    else:\n",
        "        print(\"⚠️ Embeddings file not found.\")\n",
        "\n",
        "    with open(llama_tokens_output_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        llama_lines = f.readlines()\n",
        "        print(\"Total Llama tokenized entries:\", len(llama_lines))\n",
        "        for i in range(min(2, len(llama_lines))):\n",
        "            print(llama_lines[i].strip())\n",
        "\n",
        "    if os.path.exists(invalid_lines_path):\n",
        "        with open(invalid_lines_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            invalid_lines = f.readlines()\n",
        "            print(f\"Total invalid lines: {len(invalid_lines)}\")\n",
        "            for i in range(min(2, len(invalid_lines))):\n",
        "                print(f\"Invalid line sample: {invalid_lines[i].strip()}\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Validation failed: {str(e)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 820
        },
        "id": "TvkE2tBqzczH",
        "outputId": "fdd5f96d-1b43-4555-affb-5320eacf7a6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📊 Validating embeddings...\n",
            "Total embeddings: 4514\n",
            "Embedding shape: torch.Size([4514, 768])\n",
            "\n",
            "📈 Generating t-SNE visualization...\n",
            "❌ Embedding validation failed: name 'tsne_plot_path' is not defined\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1200x800 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 10: Validate and Visualize Embeddings\n",
        "print(\"\\n📊 Validating embeddings...\")\n",
        "try:\n",
        "    if not os.path.exists(embeddings_output_path):\n",
        "        raise FileNotFoundError(f\"Embeddings file not found: {embeddings_output_path}\")\n",
        "\n",
        "    # Load embeddings\n",
        "    data = torch.load(embeddings_output_path, weights_only=False)\n",
        "    embeddings = data[\"embeddings\"]\n",
        "    embeddings_metadata = data[\"metadata\"]\n",
        "    print(f\"Total embeddings: {embeddings.shape[0]}\")\n",
        "    print(f\"Embedding shape: {embeddings.shape}\")\n",
        "\n",
        "    # Load metadata\n",
        "    with open(metadata_output_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        metadata = [json.loads(line) for line in f]\n",
        "\n",
        "    # Validate counts\n",
        "    if len(metadata) != embeddings.shape[0]:\n",
        "        raise ValueError(f\"Mismatch: {len(metadata)} metadata entries vs {embeddings.shape[0]} embeddings\")\n",
        "    if len(embeddings_metadata) != embeddings.shape[0]:\n",
        "        raise ValueError(f\"Mismatch: {len(embeddings_metadata)} metadata entries in embeddings file vs {embeddings.shape[0]} embeddings\")\n",
        "\n",
        "    # Generate t-SNE visualization\n",
        "    print(\"\\n📈 Generating t-SNE visualization...\")\n",
        "    n_samples = min(50, embeddings.shape[0])  # Use first 50 embeddings or fewer if not enough\n",
        "    if n_samples < 2:\n",
        "        print(f\"⚠️ Too few embeddings ({n_samples}) for t-SNE visualization\")\n",
        "    else:\n",
        "        tsne = TSNE(n_components=2, random_state=42, perplexity=5, max_iter=300)\n",
        "        reduced = tsne.fit_transform(embeddings[:n_samples].numpy())\n",
        "\n",
        "        # Plot\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        plt.scatter(reduced[:, 0], reduced[:, 1], alpha=0.7)\n",
        "        for i, (x, y) in enumerate(reduced):\n",
        "            plt.text(x, y, str(i), fontsize=8, alpha=0.9)\n",
        "        plt.title(\"t-SNE of First 50 Arabic Question Embeddings\")\n",
        "        plt.savefig(tsne_plot_path)\n",
        "        print(f\"💾 t-SNE plot saved to {tsne_plot_path}\")\n",
        "        plt.close()\n",
        "\n",
        "    # Print sample questions\n",
        "    print(\"\\n🔢 Question IDs and Text:\")\n",
        "    for i in range(min(5, len(metadata))):\n",
        "        print(f\"{i}: {metadata[i]['original_text']}\")\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"❌ Embedding validation failed: {str(e)}\")\n",
        "except ValueError as e:\n",
        "    print(f\"❌ Embedding validation failed: {str(e)}\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Embedding validation failed: {str(e)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JnEqN_oC3aQn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfRfeob73bUy"
      },
      "source": [
        "# Dataset Linkage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "_gCwGMGl31Da",
        "outputId": "c5c51bcc-8185-4b28-f8bd-859d6fdb55e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.7.4.2)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.4.26)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.4.1)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle) (5.29.4)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.4.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle) (0.5.1)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7c4e90e1-eb7d-4d27-8e76-ddc24bba6d65\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-7c4e90e1-eb7d-4d27-8e76-ddc24bba6d65\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "Dataset URL: https://www.kaggle.com/datasets/mazenmahmoud79/txttosql-nlp\n",
            "License(s): unknown\n",
            "AR_spider.jsonl  database\n"
          ]
        }
      ],
      "source": [
        "!pip install kaggle\n",
        "from google.colab import files\n",
        "\n",
        "# Upload Kaggle API key (you only need to do this once)\n",
        "files.upload()\n",
        "\n",
        "\n",
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "\n",
        "!kaggle datasets download -d mazenmahmoud79/txttosql-nlp\n",
        "\n",
        "\n",
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile(\"txttosql-nlp.zip\", \"r\") as zip_ref:\n",
        "    zip_ref.extractall(\"data\")  # Extract files into 'data' folder\n",
        "\n",
        "# List extracted files\n",
        "!ls data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EHVXUdlx4AtR"
      },
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "!pip install -q sqlparse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Bwrbf0a3g1q",
        "outputId": "1cc41814-9172-4177-bcf3-4be9c5f10e0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "⚠️ Missing schema for voter_1\n",
            "✅ Processed 10 databases...\n",
            "⚠️ Missing schema for flight_4\n",
            "✅ Processed 20 databases...\n",
            "⚠️ Missing schema for twitter_1\n",
            "⚠️ Missing schema for inn_1\n",
            "✅ Processed 30 databases...\n",
            "✅ Processed 40 databases...\n",
            "⚠️ Missing schema for world_1\n",
            "✅ Processed 50 databases...\n",
            "⚠️ Missing schema for student_1\n",
            "⚠️ Missing schema for epinions_1\n",
            "⚠️ Missing schema for car_1\n",
            "⚠️ Missing schema for company_1\n",
            "✅ Processed 60 databases...\n",
            "⚠️ Missing schema for wine_1\n",
            "⚠️ Missing schema for chinook_1\n",
            "✅ Processed 70 databases...\n",
            "⚠️ Missing schema for college_1\n",
            "✅ Processed 80 databases...\n",
            "⚠️ Missing schema for icfp_1\n",
            "✅ Processed 90 databases...\n",
            "⚠️ Missing schema for college_2\n",
            "✅ Processed 100 databases...\n",
            "⚠️ Missing schema for formula_1\n",
            "⚠️ Missing schema for small_bank_1\n",
            "✅ Processed 110 databases...\n",
            "⚠️ Missing schema for flight_2\n",
            "✅ Processed 120 databases...\n",
            "✅ Processed 130 databases...\n",
            "⚠️ Missing schema for wta_1\n",
            "✅ Processed 140 databases...\n",
            "\n",
            "🎉 Done: Processed 148 databases\n",
            "🚫 Missing schema files: 18\n",
            "\n",
            "🔍 Sample schema prompt:\n",
            "Table `Addresses` has columns: CREATE, address_id, line_1_number_building, town_city, zip_postcode, state_province_county, country\n",
            "Table `Services` has columns: CREATE, service_id, service_type_code, NOT, service_name, service_descriptio\n",
            "Table `Forms` has columns: CREATE, form_id, form_type_code, NOT, service_id, form_number, form_name, form_description, FOREIGN\n",
            "Table `Individuals` has columns: CREATE, individual_id, individual_first_name, individual_middle_name, inidividual_phone, individual_email, individual_address, individual_last_name\n",
            "Table `Organizations` has columns: CREATE, organization_id, date_formed, organization_name, uk_vat_number\n",
            "Table `Parties` has columns: CREATE, party_id, payment_method_code, NOT, party_phone, party_email\n",
            "Table `Organization_Contact_Individuals` has columns: CREATE, individual_id, organization_id, date_contact_from, date_contact_to, PRIMARY, FOREIGN, FOREIGN\n",
            "Table `Party_Addresses` has columns: CREATE, party_id, address_id, date_address_from, address_type_code, NOT, date_address_to, PRIMARY, FOREIGN, FOREIGN\n",
            "Table `Party_Forms` has columns: CREATE, party_id, form_id, date_completion_started, form_status_code, NOT, date_fully_completed, PRIMARY, FOREIGN, FOREIGN\n",
            "Table `Party_Services` has columns: CREATE, booking_id, customer_id, service_id, service_datetime, booking_made_date, FOREIGN, FOREIGN\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import sqlparse\n",
        "import re\n",
        "\n",
        "# Define paths\n",
        "database_root = \"/content/data/database\"\n",
        "output_file = \"/content/drive/MyDrive/schema_linked/arabic_sql_schema_linked.jsonl\"\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(\"/content/drive/MyDrive/schema_linked\", exist_ok=True)\n",
        "\n",
        "def extract_schema_elements(sql_content):\n",
        "    \"\"\"Extract tables, columns, and relationships from SQL schema\"\"\"\n",
        "    schema_data = {\"tables\": {}, \"foreign_keys\": [], \"primary_keys\": {}}\n",
        "\n",
        "    # Parse SQL statements\n",
        "    parsed = sqlparse.parse(sql_content)\n",
        "\n",
        "    for stmt in parsed:\n",
        "        if \"create table\" in str(stmt).lower():\n",
        "            # Extract table name\n",
        "            create_table = str(stmt).replace(\"\\n\", \" \")\n",
        "            table_match = re.search(r\"CREATE\\s+TABLE\\s+(?:IF\\s+NOT\\s+EXISTS\\s+)?(?:`|\\\"|)(\\w+)(?:`|\\\"|)\\s*\\(\", create_table, re.IGNORECASE)\n",
        "\n",
        "            if table_match:\n",
        "                table_name = table_match.group(1)\n",
        "                columns = []\n",
        "                primary_key = None\n",
        "\n",
        "                # Extract columns and constraints\n",
        "                column_matches = re.findall(r\"`?(\\w+)`?\\s+(\\w+)(?:\\s+\\w+)*\", create_table)\n",
        "                for col_name, col_type in column_matches:\n",
        "                    if \"primary key\" in col_type.lower():\n",
        "                        primary_key = col_name\n",
        "                    columns.append(col_name)\n",
        "\n",
        "                schema_data[\"tables\"][table_name] = columns\n",
        "                if primary_key:\n",
        "                    schema_data[\"primary_keys\"][table_name] = primary_key\n",
        "\n",
        "                # Extract foreign keys\n",
        "                fk_matches = re.findall(r\"FOREIGN KEY\\s*$\\s*`?(\\w+)`?\\s*$\\s+REFERENCES\\s+`?(\\w+)`?\\s*$\\s*`?(\\w+)`?\", create_table, re.IGNORECASE)\n",
        "                for fk_col, ref_table, ref_col in fk_matches:\n",
        "                    schema_data[\"foreign_keys\"].append({\n",
        "                        \"from_table\": table_name,\n",
        "                        \"from_column\": fk_col,\n",
        "                        \"to_table\": ref_table,\n",
        "                        \"to_column\": ref_col\n",
        "                    })\n",
        "\n",
        "    return schema_data\n",
        "\n",
        "def format_schema_prompt(schema_data):\n",
        "    \"\"\"Convert schema data into a prompt-friendly format\"\"\"\n",
        "    prompt_parts = []\n",
        "\n",
        "    # Add tables and columns\n",
        "    for table, cols in schema_data[\"tables\"].items():\n",
        "        col_list = \", \".join(cols)\n",
        "        prompt_parts.append(f\"Table `{table}` has columns: {col_list}\")\n",
        "\n",
        "    # Add primary keys\n",
        "    if schema_data[\"primary_keys\"]:\n",
        "        pk_str = \"; \".join([f\"`{t}`.{pk}\" for t, pk in schema_data[\"primary_keys\"].items()])\n",
        "        prompt_parts.append(f\"Primary keys: {pk_str}\")\n",
        "\n",
        "    # Add foreign keys\n",
        "    if schema_data[\"foreign_keys\"]:\n",
        "        fk_strings = []\n",
        "        for fk in schema_data[\"foreign_keys\"]:\n",
        "            fk_strings.append(f\"`{fk['from_table']}`.{fk['from_column']} → `{fk['to_table']}`.{fk['to_column']}\")\n",
        "        prompt_parts.append(f\"Foreign keys: {'; '.join(fk_strings)}\")\n",
        "\n",
        "    return \"\\n\".join(prompt_parts)\n",
        "\n",
        "def process_database_folders():\n",
        "    \"\"\"Process all database folders and generate schema-linked data\"\"\"\n",
        "    total_processed = 0\n",
        "    missing_schemas = 0\n",
        "\n",
        "    with open(output_file, \"w\", encoding=\"utf-8\") as out_f:\n",
        "        # Iterate through all subfolders\n",
        "        for db_folder in os.listdir(database_root):\n",
        "            folder_path = os.path.join(database_root, db_folder)\n",
        "\n",
        "            # Skip non-directory items\n",
        "            if not os.path.isdir(folder_path):\n",
        "                continue\n",
        "\n",
        "            schema_path = os.path.join(folder_path, \"schema.sql\")\n",
        "\n",
        "            # Skip if schema.sql missing\n",
        "            if not os.path.exists(schema_path):\n",
        "                missing_schemas += 1\n",
        "                print(f\"⚠️ Missing schema for {db_folder}\")\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                # Read schema file\n",
        "                with open(schema_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                    sql_content = f.read()\n",
        "\n",
        "                # Extract schema elements\n",
        "                schema_data = extract_schema_elements(sql_content)\n",
        "                schema_prompt = format_schema_prompt(schema_data)\n",
        "\n",
        "                # Save schema + questions for this database\n",
        "                total_processed += 1\n",
        "                out_f.write(json.dumps({\n",
        "                    \"db_id\": db_folder,\n",
        "                    \"schema_prompt\": schema_prompt,\n",
        "                    \"tables\": schema_data[\"tables\"],\n",
        "                    \"foreign_keys\": schema_data[\"foreign_keys\"],\n",
        "                    \"primary_keys\": schema_data[\"primary_keys\"]\n",
        "                }, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "                if total_processed % 10 == 0:\n",
        "                    print(f\"✅ Processed {total_processed} databases...\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error processing {db_folder}: {str(e)}\")\n",
        "\n",
        "    print(f\"\\n🎉 Done: Processed {total_processed} databases\")\n",
        "    print(f\"🚫 Missing schema files: {missing_schemas}\")\n",
        "\n",
        "# Run the schema linker\n",
        "process_database_folders()\n",
        "\n",
        "# Validate output\n",
        "print(\"\\n🔍 Sample schema prompt:\")\n",
        "with open(output_file, \"r\", encoding=\"utf-8\") as f:\n",
        "    first_line = f.readline()\n",
        "    schema_prompt = json.loads(first_line)[\"schema_prompt\"]\n",
        "    print(schema_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GT74IAua_V-0"
      },
      "source": [
        "**🧩 Integration with Arabic Questions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lfyRuIt04Yzg"
      },
      "outputs": [],
      "source": [
        "# Load schema data\n",
        "schema_dict = {}\n",
        "with open(\"/content/drive/MyDrive/schema_linked/arabic_sql_schema_linked.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        data = json.loads(line)\n",
        "        schema_dict[data[\"db_id\"]] = data[\"schema_prompt\"]\n",
        "\n",
        "# Load Arabic questions\n",
        "with open(\"/content/AR_spider.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
        "    lines = [json.loads(line) for line in f]\n",
        "\n",
        "# Combine with schema and save\n",
        "linked_output_path = \"/content/drive/MyDrive/schema_linked/arabic_sql_with_schema.jsonl\"\n",
        "with open(linked_output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    for line in lines:\n",
        "        db_id = line.get(\"db_id\", \"\")\n",
        "        schema_prompt = schema_dict.get(db_id, \"No schema available\")\n",
        "\n",
        "        # Create schema-aware input\n",
        "        full_prompt = f\"Given the following database schema:\\n{schema_prompt}\\n\\nGenerate SQL for this Arabic question:\\n{line['arabic']}\"\n",
        "\n",
        "        f.write(json.dumps({\n",
        "            \"db_id\": db_id,\n",
        "            \"schema_prompt\": schema_prompt,\n",
        "            \"input_prompt\": full_prompt,\n",
        "            \"original_question\": line[\"question\"],\n",
        "            \"arabic_question\": line[\"arabic\"],\n",
        "            \"sql_query\": line[\"query\"]\n",
        "        }, ensure_ascii=False) + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXwmuFAP_PNQ",
        "outputId": "af7d259e-03b0-4888-9f9a-609b44743f56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔍 Sample schema-linked prompt:\n",
            "{\n",
            "  \"db_id\": \"department_management\",\n",
            "  \"schema_prompt\": \"Table `department` has columns: CREATE, PRIMARY\\nTable `head` has columns: CREATE, PRIMARY\\nTable `management` has columns: CREATE, PRIMARY, FOREIGN, FOREIGN\",\n",
            "  \"input_prompt\": \"Given the following database schema:\\nTable `department` has columns: CREATE, PRIMARY\\nTable `head` has columns: CREATE, PRIMARY\\nTable `management` has columns: CREATE, PRIMARY, FOREIGN, FOREIGN\\n\\nGenerate SQL for this Arabic question:\\nكم عدد رؤساء الأقسام الذين تزيد أعمارهم عن 56 سنة؟\",\n",
            "  \"original_question\": \"How many heads of the departments are older than 56 ?\",\n",
            "  \"arabic_question\": \"كم عدد رؤساء الأقسام الذين تزيد أعمارهم عن 56 سنة؟\",\n",
            "  \"sql_query\": \"SELECT count(*) FROM head WHERE age  >  56\"\n",
            "}\n",
            "{\n",
            "  \"db_id\": \"department_management\",\n",
            "  \"schema_prompt\": \"Table `department` has columns: CREATE, PRIMARY\\nTable `head` has columns: CREATE, PRIMARY\\nTable `management` has columns: CREATE, PRIMARY, FOREIGN, FOREIGN\",\n",
            "  \"input_prompt\": \"Given the following database schema:\\nTable `department` has columns: CREATE, PRIMARY\\nTable `head` has columns: CREATE, PRIMARY\\nTable `management` has columns: CREATE, PRIMARY, FOREIGN, FOREIGN\\n\\nGenerate SQL for this Arabic question:\\nاعرض قائمة بأسماء رؤساء الأقسام، مكان ميلادهم، وأعمارهم مرتبة حسب العمر.\",\n",
            "  \"original_question\": \"List the name, born state and age of the heads of departments ordered by age.\",\n",
            "  \"arabic_question\": \"اعرض قائمة بأسماء رؤساء الأقسام، مكان ميلادهم، وأعمارهم مرتبة حسب العمر.\",\n",
            "  \"sql_query\": \"SELECT name ,  born_state ,  age FROM head ORDER BY age\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# Validate output\n",
        "print(\"🔍 Sample schema-linked prompt:\")\n",
        "with open(\"/content/drive/MyDrive/schema_linked/arabic_sql_with_schema.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
        "    for _ in range(2):\n",
        "        print(json.dumps(json.loads(f.readline()), indent=2, ensure_ascii=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQ-7S8gA_iLn",
        "outputId": "4f2f8984-5f78-4b15-c1d3-0d7aa08a117f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Schema visualization saved for e_government\n"
          ]
        }
      ],
      "source": [
        "from graphviz import Digraph\n",
        "\n",
        "def visualize_schema(db_id, schema_data):\n",
        "    dot = Digraph(comment=db_id)\n",
        "    tables = schema_data.get(\"tables\", {})\n",
        "\n",
        "    # Add tables with proper label formatting\n",
        "    for table, cols in tables.items():\n",
        "        # Format columns separately to avoid f-string confusion\n",
        "        cols_str = \" | \".join(cols)\n",
        "        label = f\"{table}\\n{cols_str}\"  # Now safe and valid\n",
        "        dot.node(table, label, shape=\"box\")\n",
        "\n",
        "    # Add foreign keys\n",
        "    for fk in schema_data.get(\"foreign_keys\", []):\n",
        "        dot.edge(\n",
        "            f\"{fk['from_table']}.{fk['from_column']}\",\n",
        "            f\"{fk['to_table']}.{fk['to_column']}\",\n",
        "            arrowhead=\"crow\"\n",
        "        )\n",
        "\n",
        "    dot.render(f\"/content/{db_id}_schema.gv\", view=False)\n",
        "    print(f\"✅ Schema visualization saved for {db_id}\")\n",
        "\n",
        "# Example usage\n",
        "with open(\"/content/drive/MyDrive/schema_linked/arabic_sql_schema_linked.jsonl\", \"r\") as f:\n",
        "    sample = json.loads(next(f))\n",
        "    visualize_schema(sample[\"db_id\"], sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipwmqZyvD_7N"
      },
      "source": [
        "# Building the model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import torch\n",
        "import json\n",
        "import random\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
        "from datasets import Dataset\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "# Mount Google Drive (persistent storage)\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load and preprocess dataset\n",
        "data_path = \"/content/data/AR_spider.jsonl\"\n",
        "db_folder = \"/content/drive/MyDrive/schema_linked/arabic_sql_schema_linked.jsonl\"\n",
        "\n",
        "\n",
        "\n",
        "def load_data(file_path):\n",
        "    logger.info(f\"Loading data from {file_path}\")\n",
        "    if not os.path.exists(file_path):\n",
        "        logger.error(f\"Data file not found: {file_path}\")\n",
        "        raise FileNotFoundError(f\"Data file not found: {file_path}\")\n",
        "\n",
        "    data = []\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line_number, line in enumerate(f, 1):\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            try:\n",
        "                data.append(json.loads(line))\n",
        "            except json.JSONDecodeError as e:\n",
        "                logger.error(f\"Error decoding JSON on line {line_number}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "    logger.info(f\"Loaded {len(data)} items from {file_path}\")\n",
        "    return data\n",
        "\n",
        "def preprocess_data(data):\n",
        "    inputs = [item[\"arabic\"] for item in data]\n",
        "    targets = [item[\"query\"] for item in data]\n",
        "    return Dataset.from_dict({\"input_text\": inputs, \"target_text\": targets})\n",
        "\n",
        "dataset = preprocess_data(data)\n",
        "\n",
        "# Model setup with Drive backup\n",
        "model_name = \"t5-small\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "# Save directory in Google Drive\n",
        "drive_model_path = \"/content/drive/MyDrive/t5_arabic_text_to_sql01\"\n",
        "\n",
        "def encode_examples(examples):\n",
        "    inputs = tokenizer(examples[\"input_text\"], padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
        "    targets = tokenizer(examples[\"target_text\"], padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
        "    return {\"input_ids\": inputs[\"input_ids\"], \"attention_mask\": inputs[\"attention_mask\"], \"labels\": targets[\"input_ids\"]}\n",
        "\n",
        "dataset = dataset.map(encode_examples, batched=True)\n",
        "dataset = dataset.train_test_split(test_size=0.2)\n",
        "\n",
        "# Modified training arguments with Drive backup\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=args.output_dir,\n",
        "    num_train_epochs=args.num_epochs,\n",
        "    per_device_train_batch_size=args.batch_size,\n",
        "    per_device_eval_batch_size=args.batch_size,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    learning_rate=5e-5,  # Slightly higher for MT5\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=f\"{args.output_dir}/logs\",\n",
        "    logging_steps=10,\n",
        "    save_total_limit=2,\n",
        "    fp16=True,  # Enable FP16 for MT5\n",
        ")\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    preds = pred.predictions[0]\n",
        "    labels = pred.label_ids\n",
        "\n",
        "    if preds.ndim == 3:\n",
        "        preds = preds.argmax(axis=-1)\n",
        "\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    bleu_scores = [sentence_bleu([label.split()], pred.split()) for label, pred in zip(decoded_labels, decoded_preds)]\n",
        "    avg_bleu = sum(bleu_scores) / len(bleu_scores)\n",
        "\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
        "    rouge_scores = [scorer.score(label, pred) for label, pred in zip(decoded_labels, decoded_preds)]\n",
        "    avg_rouge1 = sum([score['rouge1'].fmeasure for score in rouge_scores]) / len(rouge_scores)\n",
        "    avg_rougeL = sum([score['rougeL'].fmeasure for score in rouge_scores]) / len(rouge_scores)\n",
        "\n",
        "    return {\"bleu\": avg_bleu, \"rouge1\": avg_rouge1, \"rougeL\": avg_rougeL}\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"test\"],\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# Train with automatic backup\n",
        "try:\n",
        "    trainer.train()\n",
        "    # Save final model to Drive\n",
        "    model.save_pretrained(drive_model_path)\n",
        "    tokenizer.save_pretrained(drive_model_path)\n",
        "\n",
        "    # Create and download zip\n",
        "    shutil.make_archive(\"t5_arabic_text_to_sql01\", 'zip', drive_model_path)\n",
        "    from google.colab import files\n",
        "    files.download(\"t5_arabic_text_to_sql01.zip\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Training interrupted: {str(e)}\")\n",
        "    print(f\"Latest model version available in Google Drive: {drive_model_path}\")\n",
        "\n",
        "print(\"Process complete! Check Google Drive for model files.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "581fe9545ed3425cadfc87a9151373db",
            "65d6929c48154dd0ba12ae03c095ad9f",
            "cf1d51cf646a4aa9a46968aec43e2bc9",
            "caf9deadbbd14b86a07a681b1be2f3d0",
            "5b7b456f668f47a2a0ad0af38eb543d7",
            "e02c5c08e2004fe29f85d069d6d87c95",
            "5f3b063eca5047f2b3f57b94ea9d7307",
            "5873494764ab47e59ce038d34f2dfbc1",
            "47583a42fe384b09a5585048b594a709",
            "ad1d72b967f44439a3b6f43b8d11312e",
            "fe8c4310e73d4d99b3103dbc4e533eb1"
          ]
        },
        "id": "YvEBv8LfPLKm",
        "outputId": "096f1323-40c5-4cfc-9abb-07095ea7ce95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "581fe9545ed3425cadfc87a9151373db"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4000' max='4000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [4000/4000 12:19, Epoch 20/20]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Bleu</th>\n",
              "      <th>Rouge1</th>\n",
              "      <th>Rougel</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.133600</td>\n",
              "      <td>0.856040</td>\n",
              "      <td>0.004867</td>\n",
              "      <td>0.434833</td>\n",
              "      <td>0.392605</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.887200</td>\n",
              "      <td>0.548412</td>\n",
              "      <td>0.020635</td>\n",
              "      <td>0.559895</td>\n",
              "      <td>0.526222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.802500</td>\n",
              "      <td>0.454247</td>\n",
              "      <td>0.068486</td>\n",
              "      <td>0.627453</td>\n",
              "      <td>0.599880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.496800</td>\n",
              "      <td>0.404925</td>\n",
              "      <td>0.088470</td>\n",
              "      <td>0.648084</td>\n",
              "      <td>0.621860</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.400300</td>\n",
              "      <td>0.374011</td>\n",
              "      <td>0.099920</td>\n",
              "      <td>0.663458</td>\n",
              "      <td>0.638135</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.466500</td>\n",
              "      <td>0.351302</td>\n",
              "      <td>0.105701</td>\n",
              "      <td>0.675883</td>\n",
              "      <td>0.652041</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.414700</td>\n",
              "      <td>0.337427</td>\n",
              "      <td>0.117066</td>\n",
              "      <td>0.683207</td>\n",
              "      <td>0.659501</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.432200</td>\n",
              "      <td>0.324272</td>\n",
              "      <td>0.132700</td>\n",
              "      <td>0.696063</td>\n",
              "      <td>0.674373</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.348300</td>\n",
              "      <td>0.315981</td>\n",
              "      <td>0.127732</td>\n",
              "      <td>0.698913</td>\n",
              "      <td>0.675984</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.377300</td>\n",
              "      <td>0.307808</td>\n",
              "      <td>0.134300</td>\n",
              "      <td>0.703688</td>\n",
              "      <td>0.681341</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.364100</td>\n",
              "      <td>0.301142</td>\n",
              "      <td>0.137588</td>\n",
              "      <td>0.714780</td>\n",
              "      <td>0.692325</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.398900</td>\n",
              "      <td>0.294933</td>\n",
              "      <td>0.149733</td>\n",
              "      <td>0.716770</td>\n",
              "      <td>0.693690</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.324300</td>\n",
              "      <td>0.290362</td>\n",
              "      <td>0.153995</td>\n",
              "      <td>0.718868</td>\n",
              "      <td>0.697712</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.405300</td>\n",
              "      <td>0.287121</td>\n",
              "      <td>0.157831</td>\n",
              "      <td>0.723645</td>\n",
              "      <td>0.700049</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.267300</td>\n",
              "      <td>0.283598</td>\n",
              "      <td>0.165142</td>\n",
              "      <td>0.726975</td>\n",
              "      <td>0.704727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.338200</td>\n",
              "      <td>0.280735</td>\n",
              "      <td>0.171729</td>\n",
              "      <td>0.730223</td>\n",
              "      <td>0.708255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.286700</td>\n",
              "      <td>0.278825</td>\n",
              "      <td>0.173856</td>\n",
              "      <td>0.729348</td>\n",
              "      <td>0.707402</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.328700</td>\n",
              "      <td>0.277972</td>\n",
              "      <td>0.174531</td>\n",
              "      <td>0.732495</td>\n",
              "      <td>0.709648</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.330300</td>\n",
              "      <td>0.277195</td>\n",
              "      <td>0.175164</td>\n",
              "      <td>0.733518</td>\n",
              "      <td>0.711566</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.356700</td>\n",
              "      <td>0.276959</td>\n",
              "      <td>0.176440</td>\n",
              "      <td>0.732925</td>\n",
              "      <td>0.710538</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_209483bf-243d-4b0f-b31d-93a54f4fa08f\", \"t5_arabic_text_to_sql01.zip\", 890339302)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Process complete! Check Google Drive for model files.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, MarianMTModel, MarianTokenizer\n",
        "import torch\n",
        "\n",
        "# Initialize models\n",
        "def load_models():\n",
        "    # Load Arabic-to-English translation model\n",
        "    trans_model = MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-ar-en\").to('cuda')\n",
        "    trans_tokenizer = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-ar-en\")\n",
        "\n",
        "    # Load English-to-SQL model\n",
        "    sql_tokenizer = T5Tokenizer.from_pretrained(\"/content/drive/MyDrive/t5_arabic_text_to_sql01\")\n",
        "    sql_model = T5ForConditionalGeneration.from_pretrained(\"/content/drive/MyDrive/t5_arabic_text_to_sql01\").to('cuda')\n",
        "\n",
        "    return trans_model, trans_tokenizer, sql_model, sql_tokenizer\n",
        "\n",
        "# Pipeline function\n",
        "def arabic_to_sql_pipeline(arabic_text, trans_model, trans_tokenizer, sql_model, sql_tokenizer):\n",
        "    # Arabic to English\n",
        "    trans_inputs = trans_tokenizer(\n",
        "        arabic_text,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=128\n",
        "    ).to('cuda')\n",
        "\n",
        "    trans_outputs = trans_model.generate(**trans_inputs)\n",
        "    english_text = trans_tokenizer.decode(trans_outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # English to SQL\n",
        "    sql_inputs = sql_tokenizer(\n",
        "        f\"translate English to SQL: {english_text}\",\n",
        "        return_tensors=\"pt\",\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=128\n",
        "    ).to('cuda')\n",
        "\n",
        "    sql_outputs = sql_model.generate(\n",
        "        **sql_inputs,\n",
        "        max_length=128,\n",
        "        num_beams=4,\n",
        "        early_stopping=True\n",
        "    )\n",
        "\n",
        "    return sql_tokenizer.decode(sql_outputs[0], skip_special_tokens=True)\n"
      ],
      "metadata": {
        "id": "szvNDDPq37az"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    drive.mount('/content/drive')\n",
        "    trans_model, trans_tokenizer, sql_model, sql_tokenizer = load_models()\n",
        "\n",
        "    while True:\n",
        "        arabic_input = input(\"\\nEnter Arabic question (press Enter to exit): \").strip()\n",
        "        if not arabic_input:\n",
        "            print(\"Exiting...\")\n",
        "            break\n",
        "\n",
        "        try:\n",
        "            sql_query = arabic_to_sql_pipeline(\n",
        "                arabic_input,\n",
        "                trans_model,\n",
        "                trans_tokenizer,\n",
        "                sql_model,\n",
        "                sql_tokenizer\n",
        "            )\n",
        "            print(f\"\\nGenerated SQL: {sql_query}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating SQL: {str(e)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lSWa-YnOH6QD",
        "outputId": "f567824c-dd41-47f7-d7dd-851a6ad1f8fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\n",
            "Enter Arabic question (press Enter to exit): كم عدد المزارع \n",
            "\n",
            "Generated SQL: SELECT count(*) FROM farms\n",
            "\n",
            "Enter Arabic question (press Enter to exit): كم عدد لاعبي الدوري الاوروبي \n",
            "\n",
            "Generated SQL: SELECT count(*) FROM league\n",
            "\n",
            "Enter Arabic question (press Enter to exit): \n",
            "Exiting...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "import torch\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load your trained model\n",
        "model_path = \"/content/drive/MyDrive/t5_arabic_text_to_sql01\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
        "\n",
        "def generate_sql(arabic_input):\n",
        "    # Preprocess input\n",
        "    input_text = f\"translate Arabic to SQL: {arabic_input}\"\n",
        "\n",
        "    # Tokenize input\n",
        "    inputs = tokenizer(\n",
        "        input_text,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=128,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    # Generate SQL\n",
        "    outputs = model.generate(\n",
        "        input_ids=inputs.input_ids,\n",
        "        attention_mask=inputs.attention_mask,\n",
        "        max_length=128,\n",
        "        num_beams=4,\n",
        "        early_stopping=True\n",
        "    )\n",
        "\n",
        "    # Decode output\n",
        "    sql_query = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    return sql_query\n",
        "\n",
        "# Interactive test loop\n",
        "while True:\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    arabic_input = input(\"Enter Arabic sentence (or press Enter to exit): \")\n",
        "\n",
        "    if not arabic_input:\n",
        "        print(\"Exiting...\")\n",
        "        break\n",
        "\n",
        "    try:\n",
        "        sql_output = generate_sql(arabic_input)\n",
        "        print(\"\\nArabic Input:\", arabic_input)\n",
        "        print(\"Generated SQL:\", sql_output)\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating SQL: {str(e)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6Eb4kkJuVrC",
        "outputId": "71c77806-a2e4-432e-c8ef-9611001053d4",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\n",
            "==================================================\n",
            "Enter Arabic sentence (or press Enter to exit): كم مزرعة هناك؟\n",
            "\n",
            "Arabic Input: كم مزرعة هناك؟\n",
            "Generated SQL: SELECT Arabic FROM SQL\n",
            "\n",
            "==================================================\n",
            "Enter Arabic sentence (or press Enter to exit): عد المزارع\n",
            "\n",
            "Arabic Input: عد المزارع\n",
            "Generated SQL: SELECT Arabic FROM SQL\n",
            "\n",
            "==================================================\n",
            "Enter Arabic sentence (or press Enter to exit): Count the number of farms.\n",
            "\n",
            "Arabic Input: Count the number of farms.\n",
            "Generated SQL: SELECT count(*) FROM farms\n",
            "\n",
            "==================================================\n",
            "Enter Arabic sentence (or press Enter to exit): Total number of players in the top 5 leagues\n",
            "\n",
            "Arabic Input: Total number of players in the top 5 leagues\n",
            "Generated SQL: SELECT count(*) FROM player AS T1 JOIN league AS T2 ON T1.label = T2.label\n",
            "\n",
            "==================================================\n",
            "Enter Arabic sentence (or press Enter to exit): \n",
            "Exiting...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge_score"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s904U5sMKlV1",
        "outputId": "19dbaa64-1cb9-45b7-bfaf-6f798f4de6c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge_score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (4.67.1)\n",
            "Building wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=7aa68fd1d0a62420e52a24f4cb345fcb77b8fc50039fbfc78327f06956d33536\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: rouge_score\n",
            "Successfully installed rouge_score-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extra Fine tuning & Testing & Visualization"
      ],
      "metadata": {
        "id": "KcS7SETYGiwc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
        "from transformers import DataCollatorForSeq2Seq\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Optional: Login to Hugging Face Hub if needed (for private models or large models)\n",
        "HUGGINGFACE_TOKEN = \"************************\"\n",
        "login(token=HUGGINGFACE_TOKEN)\n",
        "\n",
        "# Define dataset paths\n",
        "TOKENIZED_JSONL_PATH = \"/content/drive/MyDrive/llama_processed/llama_tokenized.jsonl\"\n",
        "SCHEMA_LINKED_JSONL_PATH = \"/content/drive/MyDrive/schema_linked/arabic_sql_with_schema.jsonl\"\n",
        "\n",
        "# Define maximum sequence lengths\n",
        "MAX_INPUT_LENGTH = 512\n",
        "MAX_OUTPUT_LENGTH = 256\n",
        "\n",
        "class ArabicSQLDataset(Dataset):\n",
        "    \"\"\"\n",
        "    PyTorch Dataset for Arabic text-to-SQL examples.\n",
        "    Expects a JSONL file where each line contains:\n",
        "      - 'question' (Arabic NL query)\n",
        "      - 'sql' (ground-truth SQL)\n",
        "      - optionally 'schema' (string or dict) containing table/schema context.\n",
        "    The 'llama_tokenized.jsonl' is assumed to contain tokenized inputs (question+schema).\n",
        "    \"\"\"\n",
        "    def __init__(self, jsonl_path, tokenizer, max_input_len=512, max_output_len=256):\n",
        "        self.samples = []\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_input_len = max_input_len\n",
        "        self.max_output_len = max_output_len\n",
        "        with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                entry = json.loads(line)\n",
        "                # Combine question and schema if separate\n",
        "                if 'schema' in entry:\n",
        "                    input_text = entry['question'] + \" \" + entry['schema']\n",
        "                else:\n",
        "                    input_text = entry.get('question', \"\")\n",
        "                output_text = entry.get('sql', \"\")\n",
        "                if input_text and output_text:\n",
        "                    self.samples.append((input_text, output_text))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        input_text, output_text = self.samples[idx]\n",
        "        # Tokenize input and output\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            input_text, max_length=self.max_input_len,\n",
        "            padding='max_length', truncation=True, return_tensors=\"pt\"\n",
        "        )\n",
        "        outputs = self.tokenizer.encode_plus(\n",
        "            output_text, max_length=self.max_output_len,\n",
        "            padding='max_length', truncation=True, return_tensors=\"pt\"\n",
        "        )\n",
        "        # Flatten tensors\n",
        "        input_ids = inputs[\"input_ids\"].squeeze()\n",
        "        attention_mask = inputs[\"attention_mask\"].squeeze()\n",
        "        labels = outputs[\"input_ids\"].squeeze()\n",
        "        # Replace padding token id's of the labels by -100 so they are ignored in the loss\n",
        "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"labels\": labels\n",
        "        }\n",
        "\n",
        "# Initialize tokenizer and model (using multilingual T5 for Arabic)\n",
        "MODEL_NAME = \"google/mt5-small\"  # or use another T5 variant if preferred\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Create datasets (assumes train/val split is done externally or just use single file)\n",
        "train_dataset = ArabicSQLDataset(SCHEMA_LINKED_JSONL_PATH, tokenizer,\n",
        "                                max_input_len=MAX_INPUT_LENGTH,\n",
        "                                max_output_len=MAX_OUTPUT_LENGTH)\n",
        "# For illustration, using same file for val; in practice, split properly\n",
        "val_dataset = ArabicSQLDataset(SCHEMA_LINKED_JSONL_PATH, tokenizer,\n",
        "                              max_input_len=MAX_INPUT_LENGTH,\n",
        "                              max_output_len=MAX_OUTPUT_LENGTH)\n",
        "\n",
        "# Create DataLoaders\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=data_collator)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, collate_fn=data_collator)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 776,
          "referenced_widgets": [
            "2d506aca4bb7406d8c825dbfdc292ca3",
            "1bf84388af734749867ab2104b359d4d",
            "3eea2f141a284c8aad3ad9bead6c4d16",
            "b4ba8f4fc0ac45248de005d8d04820ea",
            "332a322e4ec447f697cd5295b4a41b6d",
            "9e24cde3a4924044a8efd72e14db7de8",
            "092eae81e49e4bbdbae4024b2de45ac9",
            "8c452a454d074d419024e4b2bc4e1a71",
            "48b6da49b8294cf1a53294aca79bbd62",
            "b2add11e14c3466997d84fa9a87465ca",
            "b17c8106bf844c9692080149f9a44303",
            "f9bb05f5010d46fc909ac35b340801c4",
            "acbdf1b496cf487499cb5d18c7996635",
            "6c159068f6bb4527b1e35e913bd0605a",
            "dda0766bc92a4441bf3c48275cab39ed",
            "72b73e5f07374d22ad7f42fea00e1a05",
            "d9e471614cbc41bebc9bdba13769577d",
            "24fe334c1728406cb3230f46b8f93367",
            "c60f34b3bb494f2e875b27740fd2ec77",
            "2f54426f0f2c4beca921b58ded452bfb",
            "d0477e3e3d8f49f5a3305d861d58fea9",
            "37a1d6267ce1438996424f5193458cc2",
            "d0f7f41100d847c4a074edf2d37d7be6",
            "aa6157fe2ee3490eb330f23f1fe03825",
            "df8a9381dec94dfc83aa897cf749ccdb",
            "a48443accfb74d658867bdd4b3f084b2",
            "2e7d49e5018c4205b32060cfeee4148a",
            "f74e4af0fdce49dcb03d81ef02df242a",
            "7fcdc7fd62ce48159d6177d6b7b50ab8",
            "4c485aa1984d46ef8874eb64790222d7",
            "1c4b580c9b8c4d7b945d26acbc7e523f",
            "dba5e54cee924bff862a79c8ef52b6c4",
            "5dea6296362f4bbfa4ef3926ca57cb96",
            "88a404797f864a9eafd85c26fc828639",
            "6fe543d518d34210aba6fa9bd80eb72a",
            "d68f30c01ad64e258f0f12c50399c5a2",
            "b7245f02e1424ac6996884e0c803c4cd",
            "6c3ef15f56494a219d350944a5109551",
            "d1e24110367044f7907abb7b91b7d30d",
            "e86361475dd14ed98485848f45bf3090",
            "086ef8b5209e4b6980842c682ca2a8ac",
            "af227bcd42ce42ed9b37387aa9ae30b9",
            "6b8c4e353f57454d954430ab4ed35054",
            "be3d775634ee46fca6fbe7c910979a77",
            "a61afdc8d24d4bf79d7844785833bfc6",
            "742f796da73844f5836b74906469310b",
            "58b63b9a7e5546f2af492d0d8547a068",
            "19ba1c28387c4a88854fc6b7d432f0ad",
            "f04152c2d6a54a23a7c6b5275b3a33b3",
            "9e558f77dd3749ddb858a05377995301",
            "e66ee4cdfc144d4ebc332addb5145231",
            "cb3a9f7071504273a08920b1ee8ae52f",
            "f42a7e92315a4693b5c2fb13fc41346a",
            "8dd613aa75224e4fa2028c88d617ba80",
            "ca1011769b2c4b01b7b296e82a39cfc2",
            "a9dbe6f4e50c4693a65b241bb5ab5dd6",
            "b65b6b622a1441e9984102cf947cfa93",
            "42578936b5ee4854bbea7e5c4d60c3f8",
            "9ac7c1f28bcc46f9aa44cb74588b4b46",
            "1dcd182e45c5490f89734699bced780f",
            "ca89ee78028446caa31084db2d49a00e",
            "19230b69fa144ce6ba8d0623c857d6d0",
            "5befd6f45c6f440b8a9d1424d705450a",
            "c961c8d112aa498e8f1c0d8755b6cf8c",
            "8541a82ab2a64782ae317d7437b234c4",
            "a3c78ccd02804064bcc96e6a9af4b8a8"
          ]
        },
        "id": "wkN-LhYDTbvW",
        "outputId": "1e2e60f2-12f1-452e-db5f-8660f4905813"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/82.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2d506aca4bb7406d8c825dbfdc292ca3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/553 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f9bb05f5010d46fc909ac35b340801c4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "spiece.model:   0%|          | 0.00/4.31M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d0f7f41100d847c4a074edf2d37d7be6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "88a404797f864a9eafd85c26fc828639"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n",
            "You are using a model of type mt5 to instantiate a model of type t5. This is not supported for all configurations of models and can yield errors.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/1.20G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a61afdc8d24d4bf79d7844785833bfc6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a9dbe6f4e50c4693a65b241bb5ab5dd6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "num_samples should be a positive integer value, but got num_samples=0",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-1a8f42e6e9e7>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;31m# Create DataLoaders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0mdata_collator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataCollatorForSeq2Seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_collator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0mval_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_collator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device, in_order)\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# map-style\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequentialSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/sampler.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    166\u001b[0m                 \u001b[0;34mf\"num_samples should be a positive integer value, but got num_samples={self.num_samples}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             )\n",
            "\u001b[0;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import sqlite3\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from datasets import Dataset\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, TrainingArguments, Trainer, EarlyStoppingCallback\n",
        "from peft import LoraConfig, get_peft_model, PeftModel\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import evaluate\n",
        "import re\n",
        "import logging\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from typing import List, Dict, Any, Tuple, Optional\n",
        "from huggingface_hub import login\n",
        "from google.colab import drive\n",
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
        "    handlers=[logging.StreamHandler(), logging.FileHandler(\"training_inference.log\")]\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Constants\n",
        "MAX_NEW_TOKENS = 256\n",
        "TOP_P = 0.9\n",
        "TEMPERATURE = 0.1\n",
        "HF_TOKEN = \"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\"  # Replace with your valid Hugging Face token\n",
        "DB_PATH = \"/content/data/database\"\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/arabic_sql_results\"\n",
        "TOKENIZED_DATA_PATH = \"/content/drive/MyDrive/llama_processed/llama_tokenized.jsonl\"\n",
        "SCHEMA_DATA_PATH = \"/content/drive/MyDrive/schema_linked/arabic_sql_with_schema.jsonl\"\n",
        "\n",
        "# Authenticate with Hugging Face\n",
        "login(token=HF_TOKEN)"
      ],
      "metadata": {
        "id": "T2uyA8g4NrgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Args:\n",
        "    base_model = \"google/mt5-small\"\n",
        "    tokenized_data = TOKENIZED_DATA_PATH\n",
        "    schema_data = SCHEMA_DATA_PATH\n",
        "    output_dir = OUTPUT_DIR\n",
        "    batch_size = 4\n",
        "    num_epochs = 0.25\n",
        "\n",
        "args = Args()\n",
        "\n",
        "def load_model_and_tokenizer(args, for_training=False):\n",
        "    \"\"\"Load model and tokenizer without quantization\"\"\"\n",
        "    logger.info(f\"Loading model from {args.base_model}\")\n",
        "    try:\n",
        "        model_kwargs = {\n",
        "            \"token\": HF_TOKEN,\n",
        "            \"device_map\": \"auto\",\n",
        "            \"trust_remote_code\": True,\n",
        "            \"attn_implementation\": \"eager\"\n",
        "        }\n",
        "\n",
        "        # Always disable quantization\n",
        "        model_kwargs[\"load_in_8bit\"] = False\n",
        "        model_kwargs[\"torch_dtype\"] = torch.float32  # Use full precision\n",
        "\n",
        "        from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "        model = AutoModelForSeq2SeqLM.from_pretrained(args.base_model, **model_kwargs)\n",
        "        tokenizer = AutoTokenizer.from_pretrained(args.base_model)\n",
        "\n",
        "        if for_training:\n",
        "            lora_config = LoraConfig(\n",
        "                r=8,\n",
        "                lora_alpha=32,\n",
        "                task_type=\"SEQ_2_SEQ_LM\",\n",
        "                target_modules=[\"SelfAttention.q\", \"SelfAttention.k\", \"SelfAttention.v\", \"SelfAttention.o\"],\n",
        "                lora_dropout=0.1,\n",
        "                bias=\"none\",\n",
        "                #gradient_checkpointing=True,\n",
        "\n",
        "            )\n",
        "            model = get_peft_model(model, lora_config)\n",
        "\n",
        "        model.eval()\n",
        "        return model, tokenizer\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to load model {args.base_model}: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def load_data(file_path):\n",
        "    \"\"\"Load JSONL data\"\"\"\n",
        "    logger.info(f\"Loading data from {file_path}\")\n",
        "    if not os.path.exists(file_path):\n",
        "        logger.error(f\"Data file not found: {file_path}\")\n",
        "        raise FileNotFoundError(f\"Data file not found: {file_path}\")\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        data = [json.loads(line) for line in f]\n",
        "    return data\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def get_schema_from_db(db_id, db_path=\"/content/data/database\"):\n",
        "    \"\"\"Extract schema from Spider database if schema file is unavailable\"\"\"\n",
        "    db_file = os.path.join(db_path, db_id, f\"{db_id}.sqlite\")\n",
        "    if not os.path.exists(db_file):\n",
        "        logger.warning(f\"Database file not found: {db_file}\")\n",
        "        return \"Schema unavailable\"\n",
        "    try:\n",
        "        conn = sqlite3.connect(db_file)\n",
        "        cursor = conn.cursor()\n",
        "        cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
        "        tables = cursor.fetchall()\n",
        "        schema = []\n",
        "        for table in tables:\n",
        "            table_name = table[0]\n",
        "            cursor.execute(f\"PRAGMA table_info({table_name});\")\n",
        "            columns = cursor.fetchall()\n",
        "            column_info = [f\"{col[1]} ({col[2]})\" for col in columns]\n",
        "            schema.append(f\"Table {table_name}: {', '.join(column_info)}\")\n",
        "        conn.close()\n",
        "        return \"\\n\".join(schema)\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error extracting schema for {db_id}: {str(e)}\")\n",
        "        return \"Schema unavailable\"\n"
      ],
      "metadata": {
        "id": "frvVKROsOUBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_tokenized_and_schema_data(tokenized_data, schema_data=None):\n",
        "    \"\"\"Merge tokenized data with schema and SQL references\"\"\"\n",
        "    schema_dict = {}\n",
        "\n",
        "    # Build schema dictionary by db_id\n",
        "    if schema_data:\n",
        "        for item in schema_data:\n",
        "            db_id = item.get(\"db_id\")\n",
        "            if db_id is None:\n",
        "                logger.warning(f\"Schema item missing db_id: {item}\")\n",
        "                continue\n",
        "            if \"sql_query\" not in item or not item[\"sql_query\"]:\n",
        "                logger.warning(f\"Schema item {db_id} missing sql_query\")\n",
        "                continue\n",
        "            if db_id not in schema_dict:\n",
        "                schema_dict[db_id] = []\n",
        "            schema_dict[db_id].append(item)\n",
        "\n",
        "    merged_data = []\n",
        "    for item in tokenized_data:\n",
        "        merged_item = item.copy()\n",
        "        db_id = item.get(\"db_id\")\n",
        "\n",
        "        if not db_id:\n",
        "            logger.warning(f\"Tokenized item missing db_id: {item}\")\n",
        "            continue\n",
        "\n",
        "        matched_schema_items = schema_dict.get(db_id, [])\n",
        "\n",
        "        # Match by normalized or original question\n",
        "        match = None\n",
        "        for schema_item in matched_schema_items:\n",
        "            schema_arabic = schema_item.get(\"arabic_question\", \"\").strip()\n",
        "            schema_english = schema_item.get(\"original_question\", \"\").strip()\n",
        "            item_arabic = item.get(\"original_text\", \"\").strip()\n",
        "            item_english = item.get(\"question\", \"\").strip()\n",
        "            if (schema_arabic and schema_arabic == item_arabic) or \\\n",
        "               (schema_english and schema_english == item_english):\n",
        "                match = schema_item\n",
        "                break\n",
        "\n",
        "        if match:\n",
        "            merged_item[\"schema\"] = match.get(\"schema_prompt\", \"Schema unavailable\")\n",
        "            merged_item[\"sql\"] = match.get(\"sql_query\", \"\")\n",
        "        else:\n",
        "            # Fallback to database schema if no match found\n",
        "            merged_item[\"schema\"] = get_schema_from_db(db_id)\n",
        "            merged_item[\"sql\"] = \"\"\n",
        "            logger.warning(f\"No matching schema for db_id '{db_id}' and question: {item.get('original_text')}\")\n",
        "\n",
        "        # Include item even if SQL is missing, to avoid skipping valid tokenized data\n",
        "        merged_data.append(merged_item)\n",
        "\n",
        "    return merged_data\n",
        "\n",
        "\n",
        "\n",
        "from transformers import DataCollatorForSeq2Seq\n",
        "def tokenize_function(example):\n",
        "    input_prompt = f\"\"\"### قاعدة البيانات:\n",
        "{example['schema']}\n",
        "### السؤال بالعربية:\n",
        "{example['normalized_text']}\n",
        "### استعلام SQL:\"\"\"\n",
        "    sql_query = example['sql']\n",
        "\n",
        "    # Tokenize encoder input (prompt)\n",
        "    input_encodings = tokenizer(\n",
        "        input_prompt, truncation=False, padding=False, return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    # Tokenize decoder labels (SQL query)\n",
        "    label_encodings = tokenizer(\n",
        "        sql_query, truncation=False, padding=False, return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    # Set labels to SQL token IDs\n",
        "    input_encodings[\"input_ids\"] = input_encodings[\"input_ids\"].squeeze(0)\n",
        "    input_encodings[\"attention_mask\"] = input_encodings[\"attention_mask\"].squeeze(0)\n",
        "    input_encodings[\"labels\"] = label_encodings[\"input_ids\"].squeeze(0)\n",
        "\n",
        "    return input_encodings\n",
        "\n",
        "def prepare_dataset(data):\n",
        "    dataset = Dataset.from_list(data)\n",
        "    tokenized_dataset = dataset.map(tokenize_function, batched=False, remove_columns=dataset.column_names)\n",
        "    return tokenized_dataset\n",
        "\n",
        "\n",
        "from transformers import DataCollatorForSeq2Seq, TrainingArguments, Trainer\n",
        "# ✅ Correct import\n",
        "from transformers import EarlyStoppingCallback\n",
        "def fine_tune_model(model, tokenizer, train_data, args):\n",
        "    \"\"\"Fine-tune the model with learning rate scheduling and early stopping\"\"\"\n",
        "    logger.info(\"Starting fine-tuning...\")\n",
        "    train_dataset = prepare_dataset(train_data)\n",
        "    train_dataset = train_dataset.train_test_split(test_size=0.1)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=args.output_dir,\n",
        "        num_train_epochs=args.num_epochs,\n",
        "        per_device_train_batch_size=args.batch_size,\n",
        "        per_device_eval_batch_size=args.batch_size,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"eval_loss\",\n",
        "        greater_is_better=False,\n",
        "        learning_rate=2e-5,\n",
        "        weight_decay=0.01,\n",
        "        fp16=False,\n",
        "        logging_dir=f\"{args.output_dir}/logs\",\n",
        "        logging_steps=10,\n",
        "        save_total_limit=2,\n",
        "    )\n",
        "\n",
        "    # ✅ Move DataCollator and Trainer inside this function\n",
        "    data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset[\"train\"],\n",
        "        eval_dataset=train_dataset[\"test\"],\n",
        "        data_collator=data_collator,\n",
        "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
        "    )\n",
        "\n",
        "    def configure_scheduler(optimizer):\n",
        "        return ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True)\n",
        "\n",
        "    trainer.create_optimizer()\n",
        "    trainer.lr_scheduler = configure_scheduler(trainer.optimizer)\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    model.save_pretrained(f\"{args.output_dir}/final\")\n",
        "    tokenizer.save_pretrained(f\"{args.output_dir}/final\")\n",
        "    return model, tokenizer\n",
        "\n",
        "def prepare_prompt(example):\n",
        "    \"\"\"Format prompt for Arabic-to-SQL translation with schema information\"\"\"\n",
        "    prompt = f\"\"\"### قاعدة البيانات:\n",
        "{example['schema']}\n",
        "### السؤال بالعربية:\n",
        "{example['normalized_text']}\n",
        "### استعلام SQL:\n",
        "\"\"\"\n",
        "    return prompt\n",
        "\n",
        "def generate_sql(model, tokenizer, prompts, batch_size=4, max_new_tokens=256):\n",
        "    \"\"\"Generate SQL queries with T5 model, ensuring consistent device placement\"\"\"\n",
        "    all_outputs = []\n",
        "    model.eval()\n",
        "\n",
        "    # Determine device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    logger.info(f\"Using device: {device}\")\n",
        "\n",
        "    # Move model to device explicitly\n",
        "    try:\n",
        "        model.to(device)\n",
        "        # Ensure all model parameters are on the same device\n",
        "        for param in model.parameters():\n",
        "            if param.device != device:\n",
        "                logger.warning(f\"Parameter found on {param.device}, moving to {device}\")\n",
        "                param.data = param.data.to(device)\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to move model to {device}: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "    for i in range(0, len(prompts), batch_size):\n",
        "        batch_prompts = prompts[i:i+batch_size]\n",
        "        try:\n",
        "            # Tokenize inputs\n",
        "            inputs = tokenizer(\n",
        "                batch_prompts,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                max_length=512\n",
        "            )\n",
        "\n",
        "            # Move all input tensors to the same device\n",
        "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "            # Verify input device\n",
        "            for k, v in inputs.items():\n",
        "                if v.device != device:\n",
        "                    logger.error(f\"Input tensor {k} is on {v.device}, expected {device}\")\n",
        "                    raise RuntimeError(f\"Input tensor {k} device mismatch\")\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model.generate(\n",
        "                    device = model.device if not isinstance(model.device, dict) else torch.device(\"cuda:0\"),\n",
        "                    input_ids = input_ids.to(device),\n",
        "                    attention_mask = attention_mask.to(device),\n",
        "                    max_length=max_new_tokens,\n",
        "                    num_beams=4,\n",
        "                    early_stopping=True\n",
        "                )\n",
        "\n",
        "            # Decode outputs\n",
        "            decoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "            # Clean up outputs to extract only the SQL query\n",
        "            cleaned_outputs = []\n",
        "            for output in decoded_outputs:\n",
        "                sql = output.split(\"### SQL Query:\")[-1].strip() if \"### SQL Query:\" in output else output.strip()\n",
        "                sql = re.sub(r\"<.*?>\", \"\", sql)  # Remove any remaining special tokens\n",
        "                cleaned_outputs.append(sql)\n",
        "\n",
        "            all_outputs.extend(cleaned_outputs)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error processing batch {i//batch_size}: {str(e)}\")\n",
        "            # Append empty outputs for this batch to maintain alignment\n",
        "            all_outputs.extend([\"\" for _ in batch_prompts])\n",
        "            continue\n",
        "\n",
        "    return all_outputs\n",
        "\n",
        "\n",
        "def normalize_sql(sql):\n",
        "    \"\"\"Normalize SQL query for comparison\"\"\"\n",
        "    sql = re.sub(r'\\s+', ' ', sql).strip().lower()\n",
        "    sql = sql.replace('\"', \"'\")\n",
        "    sql = re.sub(r';$', '', sql)\n",
        "    return sql\n",
        "\n",
        "def compute_exact_match(predictions, references):\n",
        "    \"\"\"Compute exact match score\"\"\"\n",
        "    matches = 0\n",
        "    for pred, ref in zip(predictions, references):\n",
        "        if normalize_sql(pred) == normalize_sql(ref):\n",
        "            matches += 1\n",
        "    return matches / len(predictions) if len(predictions) > 0 else 0\n",
        "\n",
        "def compute_component_match(predictions, references):\n",
        "    \"\"\"Compute component-level match for SQL queries\"\"\"\n",
        "    components = {\n",
        "        \"select\": r\"select\\s+(.*?)\\s*(from|$)\",\n",
        "        \"from\": r\"from\\s+(.*?)\\s*(where|group by|order by|$)\",\n",
        "        \"where\": r\"where\\s+(.*?)\\s*(group by|order by|$)\",\n",
        "        \"group_by\": r\"group by\\s+(.*?)\\s*(having|order by|$)\",\n",
        "        \"order_by\": r\"order by\\s+(.*?)($)\",\n",
        "    }\n",
        "    results = {comp: {\"matches\": 0, \"total\": 0} for comp in components}\n",
        "    for pred, ref in zip(predictions, references):\n",
        "        pred = normalize_sql(pred)\n",
        "        ref = normalize_sql(ref)\n",
        "        for comp, pattern in components.items():\n",
        "            ref_match = re.search(pattern, ref)\n",
        "            if ref_match:\n",
        "                results[comp][\"total\"] += 1\n",
        "                pred_match = re.search(pattern, pred)\n",
        "                if pred_match and normalize_sql(pred_match.group(1)) == normalize_sql(ref_match.group(1)):\n",
        "                    results[comp][\"matches\"] += 1\n",
        "    component_accuracy = {comp: counts[\"matches\"] / counts[\"total\"] if counts[\"total\"] > 0 else 0.0 for comp, counts in results.items()}\n",
        "    return component_accuracy\n",
        "\n",
        "def compute_bleu_score(predictions, references):\n",
        "    \"\"\"Compute BLEU score with empty input handling\"\"\"\n",
        "    valid_predictions = []\n",
        "    valid_references = []\n",
        "\n",
        "    for pred, ref in zip(predictions, references):\n",
        "        pred_tokens = pred.strip().split()\n",
        "        ref_tokens = ref.strip().split()\n",
        "\n",
        "        if not pred_tokens or not ref_tokens:\n",
        "            continue\n",
        "\n",
        "        valid_predictions.append(pred_tokens)\n",
        "        valid_references.append([ref_tokens])  # BLEU expects list of reference lists\n",
        "\n",
        "    if not valid_predictions:\n",
        "        logger.error(\"No valid prediction-reference pairs for BLEU score\")\n",
        "        return 0.0\n",
        "\n",
        "    bleu = evaluate.load(\"bleu\")\n",
        "    result = bleu.compute(predictions=valid_predictions, references=valid_references)\n",
        "    return result[\"bleu\"]\n",
        "\n",
        "def execute_query(query, db_path):\n",
        "    \"\"\"Execute SQL query on the specified database\"\"\"\n",
        "    try:\n",
        "        conn = sqlite3.connect(db_path)\n",
        "        cursor = conn.cursor()\n",
        "        cursor.execute(query)\n",
        "        results = cursor.fetchall()\n",
        "        conn.close()\n",
        "        return results, None\n",
        "    except Exception as e:\n",
        "        return None, str(e)\n",
        "\n",
        "def compute_execution_accuracy(predictions, references, examples):\n",
        "    \"\"\"Compute execution accuracy\"\"\"\n",
        "    correct = 0\n",
        "    executable = 0\n",
        "    execution_results = []\n",
        "    for i, (pred, ref, example) in enumerate(zip(predictions, references, examples)):\n",
        "        db_id = example.get(\"db_id\", \"\")\n",
        "        db_file = os.path.join(DB_PATH, db_id, f\"{db_id}.sqlite\")\n",
        "        if not os.path.exists(db_file):\n",
        "            logger.warning(f\"Database file not found: {db_file}\")\n",
        "            execution_results.append({\n",
        "                \"index\": i, \"executable\": False, \"correct\": False, \"error\": \"Database file not found\"\n",
        "            })\n",
        "            continue\n",
        "        ref_results, ref_error = execute_query(ref, db_file)\n",
        "        if ref_error:\n",
        "            logger.warning(f\"Error executing reference query: {ref_error}\")\n",
        "            execution_results.append({\n",
        "                \"index\": i, \"executable\": False, \"correct\": False, \"error\": f\"Reference query error: {ref_error}\"\n",
        "            })\n",
        "            continue\n",
        "        pred_results, pred_error = execute_query(pred, db_file)\n",
        "        if pred_error:\n",
        "            execution_results.append({\n",
        "                \"index\": i, \"executable\": False, \"correct\": False, \"error\": pred_error\n",
        "            })\n",
        "        else:\n",
        "            executable += 1\n",
        "            is_correct = (pred_results == ref_results)\n",
        "            if is_correct:\n",
        "                correct += 1\n",
        "            execution_results.append({\n",
        "                \"index\": i, \"executable\": True, \"correct\": is_correct,\n",
        "                \"pred_results\": str(pred_results)[:100] + (\"...\" if len(str(pred_results)) > 100 else \"\"),\n",
        "                \"ref_results\": str(ref_results)[:100] + (\"...\" if len(str(pred_results)) > 100 else \"\")\n",
        "            })\n",
        "    execution_acc = correct / len(predictions) if len(predictions) > 0 else 0\n",
        "    executable_acc = executable / len(predictions) if len(predictions) > 0 else 0\n",
        "    return {\n",
        "        \"execution_accuracy\": execution_acc,\n",
        "        \"executable_percentage\": executable_acc,\n",
        "        \"execution_results\": execution_results\n",
        "    }\n",
        "\n",
        "def visualize_results(metrics, output_dir):\n",
        "    \"\"\"Create visualizations of evaluation metrics\"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Skip plotting if metrics are invalid\n",
        "    if not metrics or metrics[\"exact_match\"] is None:\n",
        "        logger.warning(\"No metrics to visualize\")\n",
        "        return\n",
        "\n",
        "    # Plot only if valid data exists\n",
        "    if metrics[\"execution_results\"]:\n",
        "        executable_count = sum(1 for res in metrics[\"execution_results\"] if res[\"executable\"])\n",
        "        correct_count = sum(1 for res in metrics[\"execution_results\"] if res.get(\"correct\", False))\n",
        "        error_count = len(metrics[\"execution_results\"]) - executable_count\n",
        "\n",
        "        # Only plot if there are valid execution results\n",
        "        if executable_count + error_count > 0:\n",
        "            plt.figure(figsize=(8, 8))\n",
        "            plt.pie(\n",
        "                [correct_count, executable_count - correct_count, error_count],\n",
        "                labels=[\"Correct Execution\", \"Incorrect Execution\", \"Execution Error\"],\n",
        "                autopct=\"%1.1f%%\",\n",
        "                colors=[\"#4CAF50\", \"#FFC107\", \"#F44336\"]\n",
        "            )\n",
        "            plt.title(\"SQL Execution Results\")\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(os.path.join(output_dir, \"execution_results.png\"))\n",
        "            plt.close()\n",
        "\n",
        "\n",
        "def evaluate_model(model, tokenizer, test_data, output_dir, batch_size=4):\n",
        "    \"\"\"Evaluate model on test data\"\"\"\n",
        "    logger.info(\"Starting model evaluation...\")\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    prompts = [prepare_prompt(example) for example in test_data]\n",
        "    predictions = generate_sql(model, tokenizer, prompts, batch_size)\n",
        "    references = [example.get(\"sql\", \"\") for example in test_data]\n",
        "    exact_match = compute_exact_match(predictions, references)\n",
        "    component_accuracy = compute_component_match(predictions, references)\n",
        "    bleu_score = compute_bleu_score(predictions, references)\n",
        "    execution_metrics = compute_execution_accuracy(predictions, references, test_data)\n",
        "    metrics = {\n",
        "        \"exact_match\": exact_match,\n",
        "        \"component_accuracy\": component_accuracy,\n",
        "        \"bleu_score\": bleu_score,\n",
        "        \"execution_accuracy\": execution_metrics[\"execution_accuracy\"],\n",
        "        \"executable_percentage\": execution_metrics[\"executable_percentage\"],\n",
        "        \"execution_results\": execution_metrics[\"execution_results\"]\n",
        "    }\n",
        "    logger.info(f\"Evaluation metrics:\")\n",
        "    logger.info(f\"  Exact Match: {exact_match:.4f}\")\n",
        "    logger.info(f\"  BLEU Score: {bleu_score:.4f}\")\n",
        "    logger.info(f\"  Execution Accuracy: {execution_metrics['execution_accuracy']:.4f}\")\n",
        "    logger.info(f\"  Executable Percentage: {execution_metrics['executable_percentage']:.4f}\")\n",
        "    logger.info(f\"  Component Accuracy: {component_accuracy}\")\n",
        "    with open(os.path.join(output_dir, \"metrics.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(metrics, f, ensure_ascii=False, indent=2)\n",
        "    results = []\n",
        "    for i, (pred, ref, example) in enumerate(zip(predictions, references, test_data)):\n",
        "        result = {\n",
        "            \"id\": example.get(\"id\", str(i)),\n",
        "            \"db_id\": example.get(\"db_id\", \"\"),\n",
        "            \"arabic_text\": example.get(\"normalized_text\", \"\"),\n",
        "            \"english_question\": example.get(\"question\", \"\"),\n",
        "            \"reference_sql\": ref,\n",
        "            \"predicted_sql\": pred,\n",
        "            \"exact_match\": normalize_sql(pred) == normalize_sql(ref),\n",
        "            \"execution_info\": execution_metrics[\"execution_results\"][i] if i < len(execution_metrics[\"execution_results\"]) else {}\n",
        "        }\n",
        "        results.append(result)\n",
        "    with open(os.path.join(output_dir, \"predictions.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
        "    visualize_results(metrics, output_dir)\n",
        "    return metrics, results\n",
        "\n",
        "# Load tokenized data (assumed to contain 'sql' field)\n",
        "tokenized_data = load_data(args.tokenized_data)\n",
        "schema_data = load_data(args.schema_data) if os.path.exists(args.schema_data) else None\n",
        "\n",
        "# Merge data\n",
        "merged_data = merge_tokenized_and_schema_data(tokenized_data, schema_data)\n",
        "\n",
        "# Merge data\n",
        "#merged_data = merge_tokenized_and_schema_data(tokenized_data, schema_data)\n",
        "\n",
        "# Split into train and test\n",
        "if schema_data:\n",
        "    train_size = int(0.8 * len(merged_data))\n",
        "    train_data = merged_data[:train_size]\n",
        "    test_data = merged_data[train_size:]\n",
        "else:\n",
        "    train_data = []\n",
        "    test_data = merged_data\n",
        "    logger.warning(\"No schema data provided. Using tokenized data for testing only.\")\n",
        "\n",
        "# Load model and tokenizer\n",
        "model, tokenizer = load_model_and_tokenizer(args, for_training=bool(train_data))\n",
        "\n",
        "# Fine-tune if training data is available\n",
        "if train_data:\n",
        "    model, tokenizer = fine_tune_model(model, tokenizer, train_data, args)\n",
        "    # Load fine-tuned model for inference\n",
        "    # After training, in the inference loading step:\n",
        "\n",
        "# For inference\n",
        "#quantization_config = BitsAndBytesConfig(\n",
        "#    llm_int8_threshold=6.0,  # Optional, default value\n",
        "#)\n",
        "\n",
        "# After training, load fine-tuned model for inference\n",
        "# Remove quantization config and load in full precision\n",
        "base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "    args.base_model,\n",
        "    token=HF_TOKEN,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    attn_implementation=\"eager\"\n",
        ")\n",
        "\n",
        "#inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "model.to(device)\n",
        "\n",
        "# Evaluate the model\n",
        "metrics, results = evaluate_model(model, tokenizer, test_data, args.output_dir, args.batch_size)\n",
        "logger.info(\"Evaluation completed. Results saved to %s\", args.output_dir)"
      ],
      "metadata": {
        "id": "KmWXCJqnGp-J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "28855641d47242eabe3d5db238986f08",
            "9d3e39a0bf4340209b19548e642ce47d",
            "aaa6b2777cb34d8998d25d9b6d86bcbf",
            "474948e2c4134aafabd8184a54b7a01d",
            "b20c6161fbb848798179afd4040ee627",
            "0386c95375e641f6b272d94ca91d488e",
            "89761d4e2cc740e4a2dc348aebc15a6e",
            "a777826a84a942b99f2378a123dfac24",
            "c7b187b5081f4a01862cea6591690d21",
            "47653497d6c347c5be12b1421960a7e2",
            "8e7f036b41364f8ab127481e37816db8"
          ]
        },
        "outputId": "887439cc-534c-4c82-900c-ae9935580a5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/3611 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "28855641d47242eabe3d5db238986f08"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No label_names provided for model class `PeftModelForSeq2SeqLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='204' max='204' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [204/204 00:46, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 0: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 1: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 2: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 3: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 4: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 5: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 6: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 7: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 8: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 9: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 10: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 11: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 12: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 13: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 14: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 15: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 16: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 17: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 18: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 19: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 20: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 21: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 22: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 23: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 24: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 25: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 26: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 27: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 28: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 29: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 30: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 31: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 32: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 33: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 34: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 35: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 36: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 37: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 38: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 39: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 40: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 41: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 42: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 43: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 44: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 45: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 46: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 47: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 48: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 49: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 50: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 51: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 52: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 53: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 54: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 55: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 56: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 57: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 58: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 59: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 60: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 61: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 62: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 63: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 64: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 65: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 66: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 67: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 68: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 69: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 70: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 71: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 72: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 73: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 74: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 75: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 76: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 77: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 78: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 79: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 80: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 81: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 82: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 83: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 84: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 85: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 86: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 87: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 88: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 89: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 90: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 91: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 92: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 93: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 94: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 95: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 96: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 97: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 98: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 99: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 100: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 101: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 102: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 103: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 104: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 105: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 106: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 107: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 108: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 109: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 110: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 111: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 112: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 113: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 114: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 115: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 116: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 117: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 118: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 119: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 120: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 121: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 122: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 123: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 124: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 125: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 126: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 127: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 128: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 129: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 130: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 131: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 132: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 133: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 134: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 135: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 136: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 137: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 138: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 139: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 140: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 141: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 142: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 143: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 144: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 145: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 146: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 147: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 148: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 149: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 150: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 151: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 152: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 153: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 154: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 155: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 156: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 157: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 158: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 159: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 160: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 161: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 162: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 163: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 164: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 165: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 166: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 167: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 168: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 169: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 170: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 171: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 172: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 173: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 174: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 175: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 176: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 177: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 178: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 179: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 180: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 181: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 182: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 183: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 184: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 185: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 186: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 187: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 188: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 189: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 190: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 191: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 192: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 193: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 194: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 195: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 196: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 197: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 198: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 199: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 200: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 201: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 202: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 203: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 204: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 205: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 206: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 207: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 208: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 209: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 210: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 211: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 212: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 213: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 214: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 215: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 216: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 217: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 218: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 219: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 220: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 221: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 222: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 223: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 224: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 225: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:No valid prediction-reference pairs for BLEU score\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Import libraries\n",
        "import os\n",
        "import json\n",
        "import logging\n",
        "import torch\n",
        "import re\n",
        "import sqlite3\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import evaluate\n",
        "from tqdm import tqdm\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
        "from transformers.trainer_callback import EarlyStoppingCallback\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from datasets import Dataset\n",
        "from google.colab import drive\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
        "    handlers=[logging.StreamHandler(), logging.FileHandler(\"training_inference.log\")]\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Constants\n",
        "HF_TOKEN = \"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\"  # Replace with your valid Hugging Face token\n",
        "DB_PATH = \"/content/data/database\"\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/arabic_sql_results\"\n",
        "TOKENIZED_DATA_PATH = \"/content/drive/MyDrive/llama_processed/llama_tokenized.jsonl\"\n",
        "SCHEMA_DATA_PATH = \"/content/drive/MyDrive/schema_linked/arabic_sql_with_schema.jsonl\"\n",
        "\n",
        "# Authenticate with Hugging Face\n",
        "login(token=HF_TOKEN)\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define arguments\n",
        "class Args:\n",
        "    base_model = \"t5-base\"  # Use T5-base for Seq2Seq\n",
        "    tokenized_data = TOKENIZED_DATA_PATH\n",
        "    schema_data = SCHEMA_DATA_PATH\n",
        "    output_dir = OUTPUT_DIR\n",
        "    batch_size = 4\n",
        "    num_epochs = 0.5\n",
        "\n",
        "args = Args()\n",
        "\n",
        "# Function to load JSONL data\n",
        "def load_data(file_path):\n",
        "    \"\"\"Load JSONL data\"\"\"\n",
        "    logger.info(f\"Loading data from {file_path}\")\n",
        "    if not os.path.exists(file_path):\n",
        "        logger.error(f\"Data file not found: {file_path}\")\n",
        "        raise FileNotFoundError(f\"Data file not found: {file_path}\")\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        data = [json.loads(line) for line in f]\n",
        "    return data\n",
        "\n",
        "# Function to extract schema from SQLite database\n",
        "def get_schema_from_db(db_id, db_path=\"/content/data/database\"):\n",
        "    \"\"\"Extract schema from Spider database if schema file is unavailable\"\"\"\n",
        "    db_file = os.path.join(db_path, db_id, f\"{db_id}.sqlite\")\n",
        "    if not os.path.exists(db_file):\n",
        "        logger.warning(f\"Database file not found: {db_file}\")\n",
        "        return \"Schema unavailable\"\n",
        "    try:\n",
        "        conn = sqlite3.connect(db_file)\n",
        "        cursor = conn.cursor()\n",
        "        cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
        "        tables = cursor.fetchall()\n",
        "        schema = []\n",
        "        for table in tables:\n",
        "            table_name = table[0]\n",
        "            cursor.execute(f\"PRAGMA table_info({table_name});\")\n",
        "            columns = cursor.fetchall()\n",
        "            column_info = [f\"{col[1]} ({col[2]})\" for col in columns]\n",
        "            schema.append(f\"Table {table_name}: {', '.join(column_info)}\")\n",
        "        conn.close()\n",
        "        return \"\\n\".join(schema)\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error extracting schema for {db_id}: {str(e)}\")\n",
        "        return \"Schema unavailable\"\n",
        "\n",
        "# Function to merge tokenized and schema data\n",
        "def merge_tokenized_and_schema_data(tokenized_data, schema_data=None):\n",
        "    \"\"\"Merge tokenized data with schema and SQL references\"\"\"\n",
        "    schema_dict = {}\n",
        "\n",
        "    # Build schema dictionary by db_id\n",
        "    if schema_data:\n",
        "        for item in schema_data:\n",
        "            db_id = item.get(\"db_id\")\n",
        "            if db_id is None:\n",
        "                logger.warning(f\"Schema item missing db_id: {item}\")\n",
        "                continue\n",
        "            if \"sql_query\" not in item or not item[\"sql_query\"]:\n",
        "                logger.warning(f\"Schema item {db_id} missing sql_query\")\n",
        "                continue\n",
        "            if db_id not in schema_dict:\n",
        "                schema_dict[db_id] = []\n",
        "            schema_dict[db_id].append(item)\n",
        "\n",
        "    merged_data = []\n",
        "    for item in tokenized_data:\n",
        "        merged_item = item.copy()\n",
        "        db_id = item.get(\"db_id\")\n",
        "\n",
        "        if not db_id:\n",
        "            logger.warning(f\"Tokenized item missing db_id: {item}\")\n",
        "            continue\n",
        "\n",
        "        matched_schema_items = schema_dict.get(db_id, [])\n",
        "\n",
        "        # Match by normalized or original question\n",
        "        match = None\n",
        "        for schema_item in matched_schema_items:\n",
        "            schema_arabic = schema_item.get(\"arabic_question\", \"\").strip()\n",
        "            schema_english = schema_item.get(\"original_question\", \"\").strip()\n",
        "            item_arabic = item.get(\"original_text\", \"\").strip()\n",
        "            item_english = item.get(\"question\", \"\").strip()\n",
        "            if (schema_arabic and schema_arabic == item_arabic) or \\\n",
        "               (schema_english and schema_english == item_english):\n",
        "                match = schema_item\n",
        "                break\n",
        "\n",
        "        if match:\n",
        "            merged_item[\"schema\"] = match.get(\"schema_prompt\", \"Schema unavailable\")\n",
        "            merged_item[\"sql\"] = match.get(\"sql_query\", \"\")\n",
        "        else:\n",
        "            # Fallback to database schema if no match found\n",
        "            merged_item[\"schema\"] = get_schema_from_db(db_id)\n",
        "            merged_item[\"sql\"] = \"\"\n",
        "            logger.warning(f\"No matching schema for db_id '{db_id}' and question: {item.get('original_text')}\")\n",
        "\n",
        "        # Include item even if SQL is missing\n",
        "        merged_data.append(merged_item)\n",
        "\n",
        "    return merged_data\n",
        "\n",
        "# Function to prepare prompt for T5\n",
        "def prepare_prompt(example):\n",
        "    \"\"\"Format prompt for Arabic-to-SQL translation with schema information\"\"\"\n",
        "    prompt = f\"\"\"### Database Schema:\n",
        "{example['schema']}\n",
        "\n",
        "### Arabic Question:\n",
        "{example['normalized_text']}\n",
        "\n",
        "### SQL Query:\n",
        "\"\"\"\n",
        "    return prompt\n",
        "\n",
        "# Function to generate SQL queries\n",
        "def generate_sql(model, tokenizer, prompts, batch_size=4, max_new_tokens=256):\n",
        "    \"\"\"Generate SQL queries with T5 model, ensuring consistent device placement\"\"\"\n",
        "    all_outputs = []\n",
        "    model.eval()\n",
        "\n",
        "    # Determine device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    logger.info(f\"Using device: {device}\")\n",
        "\n",
        "    # Move model to device explicitly\n",
        "    try:\n",
        "        model.to(device)\n",
        "        # Ensure all model parameters are on the same device\n",
        "        for param in model.parameters():\n",
        "            if param.device != device:\n",
        "                logger.warning(f\"Parameter found on {param.device}, moving to {device}\")\n",
        "                param.data = param.data.to(device)\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to move model to {device}: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "    for i in range(0, len(prompts), batch_size):\n",
        "        batch_prompts = prompts[i:i+batch_size]\n",
        "        try:\n",
        "            # Tokenize inputs\n",
        "            inputs = tokenizer(\n",
        "                batch_prompts,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                max_length=512\n",
        "            )\n",
        "\n",
        "            # Move all input tensors to the same device\n",
        "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "            # Verify input device\n",
        "            for k, v in inputs.items():\n",
        "                if v.device != device:\n",
        "                    logger.error(f\"Input tensor {k} is on {v.device}, expected {device}\")\n",
        "                    raise RuntimeError(f\"Input tensor {k} device mismatch\")\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model.generate(\n",
        "                    device = model.device if not isinstance(model.device, dict) else torch.device(\"cuda:0\"),\n",
        "                    input_ids = input_ids.to(device),\n",
        "                    attention_mask = attention_mask.to(device),\n",
        "                    max_length=max_new_tokens,\n",
        "                    num_beams=4,\n",
        "                    early_stopping=True\n",
        "                )\n",
        "\n",
        "            # Decode outputs\n",
        "            decoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "            # Clean up outputs to extract only the SQL query\n",
        "            cleaned_outputs = []\n",
        "            for output in decoded_outputs:\n",
        "                sql = output.split(\"### SQL Query:\")[-1].strip() if \"### SQL Query:\" in output else output.strip()\n",
        "                sql = re.sub(r\"<.*?>\", \"\", sql)  # Remove any remaining special tokens\n",
        "                cleaned_outputs.append(sql)\n",
        "\n",
        "            all_outputs.extend(cleaned_outputs)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error processing batch {i//batch_size}: {str(e)}\")\n",
        "            # Append empty outputs for this batch to maintain alignment\n",
        "            all_outputs.extend([\"\" for _ in batch_prompts])\n",
        "            continue\n",
        "\n",
        "    return all_outputs\n",
        "\n",
        "# Function to tokenize data for training\n",
        "def tokenize_function(example, tokenizer):\n",
        "    \"\"\"Tokenize inputs and labels for T5\"\"\"\n",
        "    input_prompt = f\"\"\"### Database Schema:\n",
        "{example['schema']}\n",
        "\n",
        "### Arabic Question:\n",
        "{example['normalized_text']}\n",
        "\n",
        "### SQL Query:\"\"\"\n",
        "\n",
        "    sql_query = example['sql']\n",
        "\n",
        "    # Tokenize encoder input (prompt)\n",
        "    input_encodings = tokenizer(\n",
        "        input_prompt,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=512,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    # Tokenize decoder labels (SQL query)\n",
        "    label_encodings = tokenizer(\n",
        "        sql_query,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=256,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    # Prepare encodings\n",
        "    encodings = {\n",
        "        \"input_ids\": input_encodings[\"input_ids\"].squeeze(0),\n",
        "        \"attention_mask\": input_encodings[\"attention_mask\"].squeeze(0),\n",
        "        \"labels\": label_encodings[\"input_ids\"].squeeze(0)\n",
        "    }\n",
        "\n",
        "    return encodings\n",
        "\n",
        "# Function to prepare dataset\n",
        "def prepare_dataset(data, tokenizer):\n",
        "    \"\"\"Prepare dataset for training\"\"\"\n",
        "    dataset = Dataset.from_list(data)\n",
        "    tokenized_dataset = dataset.map(\n",
        "        lambda x: tokenize_function(x, tokenizer),\n",
        "        batched=False,\n",
        "        remove_columns=dataset.column_names\n",
        "    )\n",
        "    return tokenized_dataset\n",
        "\n",
        "# Function to fine-tune T5 model\n",
        "def fine_tune_model(model, tokenizer, train_data, args):\n",
        "    \"\"\"Fine-tune T5 model with learning rate scheduling and early stopping\"\"\"\n",
        "    logger.info(\"Starting fine-tuning...\")\n",
        "\n",
        "    # Filter out items with empty SQL\n",
        "    valid_train_data = [item for item in train_data if item.get(\"sql\", \"\").strip()]\n",
        "    if not valid_train_data:\n",
        "        logger.error(\"No valid training data with SQL queries\")\n",
        "        raise ValueError(\"No valid training data with SQL queries\")\n",
        "\n",
        "    # Prepare dataset\n",
        "    train_dataset = prepare_dataset(valid_train_data, tokenizer)\n",
        "    train_dataset = train_dataset.train_test_split(test_size=0.1)\n",
        "\n",
        "    # Define training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=args.output_dir,\n",
        "        num_train_epochs=args.num_epochs,\n",
        "        per_device_train_batch_size=args.batch_size,\n",
        "        per_device_eval_batch_size=args.batch_size,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"eval_loss\",\n",
        "        greater_is_better=False,\n",
        "        learning_rate=2e-5,\n",
        "        weight_decay=0.01,\n",
        "        fp16=torch.cuda.is_available(),  # Enable FP16 if CUDA is available\n",
        "        logging_dir=f\"{args.output_dir}/logs\",\n",
        "        logging_steps=10,\n",
        "        save_total_limit=2,\n",
        "    )\n",
        "\n",
        "    # Data collator for Seq2Seq\n",
        "    data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
        "\n",
        "    # Initialize trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset[\"train\"],\n",
        "        eval_dataset=train_dataset[\"test\"],\n",
        "        data_collator=data_collator,\n",
        "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
        "    )\n",
        "\n",
        "    # Configure learning rate scheduler\n",
        "    def configure_scheduler(optimizer):\n",
        "        return ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True)\n",
        "\n",
        "    trainer.create_optimizer()\n",
        "    trainer.lr_scheduler = configure_scheduler(trainer.optimizer)\n",
        "\n",
        "    # Train the model\n",
        "    trainer.train()\n",
        "\n",
        "    # Save the model and tokenizer\n",
        "    model.save_pretrained(f\"{args.output_dir}/final\")\n",
        "    tokenizer.save_pretrained(f\"{args.output_dir}/final\")\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "# Evaluation functions\n",
        "def normalize_sql(sql):\n",
        "    \"\"\"Normalize SQL query for comparison\"\"\"\n",
        "    if not sql:\n",
        "        return \"\"\n",
        "    sql = re.sub(r'\\s+', ' ', sql).strip().lower()\n",
        "    sql = sql.replace('\"', \"'\")\n",
        "    sql = re.sub(r';$', '', sql)\n",
        "    return sql\n",
        "\n",
        "def compute_exact_match(predictions, references):\n",
        "    \"\"\"Compute exact match score\"\"\"\n",
        "    matches = 0\n",
        "    valid_pairs = 0\n",
        "    for pred, ref in zip(predictions, references):\n",
        "        if not ref or not pred:\n",
        "            continue\n",
        "        valid_pairs += 1\n",
        "        if normalize_sql(pred) == normalize_sql(ref):\n",
        "            matches += 1\n",
        "    return matches / valid_pairs if valid_pairs > 0 else 0\n",
        "\n",
        "def compute_component_match(predictions, references):\n",
        "    \"\"\"Compute component-level match for SQL queries\"\"\"\n",
        "    components = {\n",
        "        \"select\": r\"select\\s+(.*?)\\s*(from|$)\",\n",
        "        \"from\": r\"from\\s+(.*?)\\s*(where|group by|order by|$)\",\n",
        "        \"where\": r\"where\\s+(.*?)\\s*(group by|order by|$)\",\n",
        "        \"group_by\": r\"group by\\s+(.*?)\\s*(having|order by|$)\",\n",
        "        \"order_by\": r\"order by\\s+(.*?)$\",\n",
        "    }\n",
        "    results = {comp: {\"matches\": 0, \"total\": 0} for comp in components}\n",
        "    for pred, ref in zip(predictions, references):\n",
        "        pred = normalize_sql(pred)\n",
        "        ref = normalize_sql(ref)\n",
        "        if not pred or not ref:\n",
        "            continue\n",
        "        for comp, pattern in components.items():\n",
        "            ref_match = re.search(pattern, ref)\n",
        "            if ref_match:\n",
        "                results[comp][\"total\"] += 1\n",
        "                pred_match = re.search(pattern, pred)\n",
        "                if pred_match and normalize_sql(pred_match.group(1)) == normalize_sql(ref_match.group(1)):\n",
        "                    results[comp][\"matches\"] += 1\n",
        "    component_accuracy = {comp: counts[\"matches\"] / counts[\"total\"] if counts[\"total\"] > 0 else 0.0 for comp, counts in results.items()}\n",
        "    return component_accuracy\n",
        "\n",
        "def compute_bleu_score(predictions, references):\n",
        "    \"\"\"Compute BLEU score with empty input handling\"\"\"\n",
        "    valid_predictions = []\n",
        "    valid_references = []\n",
        "    for pred, ref in zip(predictions, references):\n",
        "        pred_tokens = pred.strip().split() if pred else []\n",
        "        ref_tokens = ref.strip().split() if ref else []\n",
        "        if not pred_tokens or not ref_tokens:\n",
        "            continue\n",
        "        valid_predictions.append(pred_tokens)\n",
        "        valid_references.append([ref_tokens])\n",
        "    if not valid_predictions:\n",
        "        logger.warning(\"No valid prediction-reference pairs for BLEU score\")\n",
        "        return 0.0\n",
        "    bleu = evaluate.load(\"bleu\")\n",
        "    result = bleu.compute(predictions=valid_predictions, references=valid_references)\n",
        "    return result[\"bleu\"]\n",
        "\n",
        "def execute_query(query, db_path):\n",
        "    \"\"\"Execute SQL query on the specified database\"\"\"\n",
        "    try:\n",
        "        conn = sqlite3.connect(db_path)\n",
        "        cursor = conn.cursor()\n",
        "        cursor.execute(query)\n",
        "        results = cursor.fetchall()\n",
        "        conn.close()\n",
        "        return results, None\n",
        "    except Exception as e:\n",
        "        return None, str(e)\n",
        "\n",
        "def compute_execution_accuracy(predictions, references, examples):\n",
        "    \"\"\"Compute execution accuracy\"\"\"\n",
        "    correct = 0\n",
        "    executable = 0\n",
        "    execution_results = []\n",
        "    for i, (pred, ref, example) in enumerate(zip(predictions, references, examples)):\n",
        "        db_id = example.get(\"db_id\", \"\")\n",
        "        db_file = os.path.join(\"/content/data/database\", db_id, f\"{db_id}.sqlite\")\n",
        "        if not os.path.exists(db_file):\n",
        "            logger.warning(f\"Database file not found: {db_file}\")\n",
        "            execution_results.append({\n",
        "                \"index\": i, \"executable\": False, \"correct\": False, \"error\": \"Database file not found\"\n",
        "            })\n",
        "            continue\n",
        "        if not ref:\n",
        "            logger.warning(f\"Reference SQL empty for example {i}\")\n",
        "            execution_results.append({\n",
        "                \"index\": i, \"executable\": False, \"correct\": False, \"error\": \"Empty reference SQL\"\n",
        "            })\n",
        "            continue\n",
        "        ref_results, ref_error = execute_query(ref, db_file)\n",
        "        if ref_error:\n",
        "            logger.warning(f\"Error executing reference query: {ref_error}\")\n",
        "            execution_results.append({\n",
        "                \"index\": i, \"executable\": False, \"correct\": False, \"error\": f\"Reference query error: {ref_error}\"\n",
        "            })\n",
        "            continue\n",
        "        pred_results, pred_error = execute_query(pred, db_file)\n",
        "        if pred_error:\n",
        "            execution_results.append({\n",
        "                \"index\": i, \"executable\": False, \"correct\": False, \"error\": pred_error\n",
        "            })\n",
        "        else:\n",
        "            executable += 1\n",
        "            is_correct = (pred_results == ref_results)\n",
        "            if is_correct:\n",
        "                correct += 1\n",
        "            execution_results.append({\n",
        "                \"index\": i, \"executable\": True, \"correct\": is_correct,\n",
        "                \"pred_results\": str(pred_results)[:100] + (\"...\" if len(str(pred_results)) > 100 else \"\"),\n",
        "                \"ref_results\": str(ref_results)[:100] + (\"...\" if len(str(pred_results)) > 100 else \"\")\n",
        "            })\n",
        "    execution_acc = correct / len(predictions) if len(predictions) > 0 else 0\n",
        "    executable_acc = executable / len(predictions) if len(predictions) > 0 else 0\n",
        "    return {\n",
        "        \"execution_accuracy\": execution_acc,\n",
        "        \"executable_percentage\": executable_acc,\n",
        "        \"execution_results\": execution_results\n",
        "    }\n",
        "\n",
        "def visualize_results(metrics, output_dir):\n",
        "    \"\"\"Create visualizations of evaluation metrics\"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Overall metrics plot\n",
        "    overall_metrics = {\n",
        "        \"Exact Match\": metrics.get(\"exact_match\", 0.0),\n",
        "        \"BLEU Score\": metrics.get(\"bleu_score\", 0.0),\n",
        "        \"Execution Accuracy\": metrics.get(\"execution_accuracy\", 0.0),\n",
        "        \"Executable %\": metrics.get(\"executable_percentage\", 0.0)\n",
        "    }\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.barplot(x=list(overall_metrics.keys()), y=list(overall_metrics.values()))\n",
        "    plt.title(\"Overall Model Performance Metrics\")\n",
        "    plt.ylabel(\"Score\")\n",
        "    plt.ylim(0, 1)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(output_dir, \"overall_metrics.png\"))\n",
        "    plt.close()\n",
        "\n",
        "    # Component accuracy plot\n",
        "    component_acc = metrics.get(\"component_accuracy\", {})\n",
        "    if component_acc:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.barplot(x=list(component_acc.keys()), y=list(component_acc.values()))\n",
        "        plt.title(\"SQL Component-Level Accuracy\")\n",
        "        plt.ylabel(\"Accuracy\")\n",
        "        plt.ylim(0, 1)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(output_dir, \"component_accuracy.png\"))\n",
        "        plt.close()\n",
        "\n",
        "    # Execution results pie chart\n",
        "    execution_results = metrics.get(\"execution_results\", [])\n",
        "    if execution_results:\n",
        "        executable_count = sum(1 for res in execution_results if res[\"executable\"])\n",
        "        correct_count = sum(1 for res in execution_results if res.get(\"correct\", False))\n",
        "        error_count = len(execution_results) - executable_count\n",
        "        if executable_count + error_count > 0:\n",
        "            plt.figure(figsize=(8, 8))\n",
        "            plt.pie(\n",
        "                [correct_count, executable_count - correct_count, error_count],\n",
        "                labels=[\"Correct Execution\", \"Incorrect Execution\", \"Execution Error\"],\n",
        "                autopct=\"%1.1f%%\",\n",
        "                colors=[\"#4CAF50\", \"#FFC107\", \"#F44336\"]\n",
        "            )\n",
        "            plt.title(\"SQL Execution Results\")\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(os.path.join(output_dir, \"execution_results.png\"))\n",
        "            plt.close()\n",
        "\n",
        "def evaluate_model(model, tokenizer, test_data, output_dir, batch_size=4):\n",
        "    \"\"\"Evaluate model on test data\"\"\"\n",
        "    logger.info(\"Starting model evaluation...\")\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    prompts = [prepare_prompt(example) for example in test_data]\n",
        "    predictions = generate_sql(model, tokenizer, prompts, batch_size)\n",
        "    references = [example.get(\"sql\", \"\") for example in test_data]\n",
        "\n",
        "    exact_match = compute_exact_match(predictions, references)\n",
        "    component_accuracy = compute_component_match(predictions, references)\n",
        "    bleu_score = compute_bleu_score(predictions, references)\n",
        "    execution_metrics = compute_execution_accuracy(predictions, references, test_data)\n",
        "\n",
        "    metrics = {\n",
        "        \"exact_match\": exact_match,\n",
        "        \"component_accuracy\": component_accuracy,\n",
        "        \"bleu_score\": bleu_score,\n",
        "        \"execution_accuracy\": execution_metrics[\"execution_accuracy\"],\n",
        "        \"executable_percentage\": execution_metrics[\"executable_percentage\"],\n",
        "        \"execution_results\": execution_metrics[\"execution_results\"]\n",
        "    }\n",
        "\n",
        "    logger.info(f\"Evaluation metrics:\")\n",
        "    logger.info(f\"  Exact Match: {exact_match:.4f}\")\n",
        "    logger.info(f\"  BLEU Score: {bleu_score:.4f}\")\n",
        "    logger.info(f\"  Execution Accuracy: {execution_metrics['execution_accuracy']:.4f}\")\n",
        "    logger.info(f\"  Executable Percentage: {execution_metrics['executable_percentage']:.4f}\")\n",
        "    logger.info(f\"  Component Accuracy: {component_accuracy}\")\n",
        "\n",
        "    with open(os.path.join(output_dir, \"metrics.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(metrics, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    results = []\n",
        "    for i, (pred, ref, example) in enumerate(zip(predictions, references, test_data)):\n",
        "        result = {\n",
        "            \"id\": example.get(\"id\", str(i)),\n",
        "            \"db_id\": example.get(\"db_id\", \"\"),\n",
        "            \"arabic_text\": example.get(\"normalized_text\", \"\"),\n",
        "            \"english_question\": example.get(\"question\", \"\"),\n",
        "            \"reference_sql\": ref,\n",
        "            \"predicted_sql\": pred,\n",
        "            \"exact_match\": normalize_sql(pred) == normalize_sql(ref) if ref and pred else False,\n",
        "            \"execution_info\": execution_metrics[\"execution_results\"][i] if i < len(execution_metrics[\"execution_results\"]) else {}\n",
        "        }\n",
        "        results.append(result)\n",
        "\n",
        "    with open(os.path.join(output_dir, \"predictions.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    visualize_results(metrics, output_dir)\n",
        "    return metrics, results\n",
        "\n",
        "def load_model_and_tokenizer(args, for_training=False):\n",
        "    \"\"\"Load model and tokenizer without quantization\"\"\"\n",
        "    from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "    from peft import LoraConfig, get_peft_model  # ✅ Import LoraConfig and get_peft_model\n",
        "\n",
        "    logger.info(f\"Loading model from {args.base_model}\")\n",
        "    try:\n",
        "        model_kwargs = {\n",
        "            \"token\": HF_TOKEN,\n",
        "            \"device_map\": \"auto\",\n",
        "            \"trust_remote_code\": True,\n",
        "            \"attn_implementation\": \"eager\"\n",
        "        }\n",
        "\n",
        "        # Always disable quantization\n",
        "        model_kwargs[\"load_in_8bit\"] = False\n",
        "        model_kwargs[\"torch_dtype\"] = torch.float32  # Use full precision\n",
        "\n",
        "        model = AutoModelForSeq2SeqLM.from_pretrained(args.base_model, **model_kwargs)\n",
        "        tokenizer = AutoTokenizer.from_pretrained(args.base_model)\n",
        "\n",
        "        if for_training:\n",
        "            lora_config = LoraConfig(\n",
        "                r=8,\n",
        "                lora_alpha=32,\n",
        "                target_modules=[\"q\", \"v\"],\n",
        "                lora_dropout=0.1,\n",
        "                bias=\"none\",\n",
        "                task_type=\"SEQ_2_SEQ_LM\",\n",
        "            )\n",
        "            model = get_peft_model(model, lora_config)\n",
        "\n",
        "        model.eval()\n",
        "        return model, tokenizer\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to load model {args.base_model}: {str(e)}\")\n",
        "        raise\n",
        "# Main execution\n",
        "try:\n",
        "    # Clear GPU memory\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # Load data\n",
        "    tokenized_data = load_data(args.tokenized_data)\n",
        "    schema_data = load_data(args.schema_data) if os.path.exists(args.schema_data) else None\n",
        "\n",
        "    # Merge data\n",
        "    merged_data = merge_tokenized_and_schema_data(tokenized_data, schema_data)\n",
        "\n",
        "    # Split into train and test\n",
        "    train_size = int(0.8 * len(merged_data))\n",
        "    train_data = merged_data[:train_size]\n",
        "    test_data = merged_data[train_size:]\n",
        "\n",
        "    # Load model and tokenizer for training\n",
        "    model, tokenizer = load_model_and_tokenizer(args, for_training=True)\n",
        "\n",
        "    # Fine-tune the model\n",
        "    if train_data:\n",
        "        model, tokenizer = fine_tune_model(model, tokenizer, train_data, args)\n",
        "\n",
        "    # Load fine-tuned model for inference\n",
        "    model, tokenizer = load_model_and_tokenizer(args, for_training=False)\n",
        "\n",
        "    # Evaluate the model\n",
        "    metrics, results = evaluate_model(model, tokenizer, test_data, args.output_dir, args.batch_size)\n",
        "    logger.info(\"Evaluation completed. Results saved to %s\", args.output_dir)\n",
        "\n",
        "except Exception as e:\n",
        "    logger.error(f\"Script failed: {str(e)}\")\n",
        "    raise\n",
        "\n",
        "from peft import LoraConfig, get_peft_model, PeftModel\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "5c162f7591a547f4a8d79b7a592cf706",
            "fbab15958cfc47d997f28e27007497e1",
            "75c88558082d4d6081319bee025bffe9",
            "1b17ac15156b43e6adf0f7ed9e3c70f1",
            "7fbb55949e0345a69e088d90dc730787",
            "14179612262c40c1814169383cc1b2bb",
            "9dcc18e1b8a54d378674df750a178659",
            "9e12f71a13eb454a90ef3a9f2f78b6ab",
            "c48450c092ed41f2a6ee55e780cd5d70",
            "f6a5a08cab6a4360a729270574fbcc1d",
            "659ec6c22d6849debc151e77e0f3423f"
          ]
        },
        "id": "9Ea9-jWJZm18",
        "outputId": "aa9f05f9-bde3-43f0-dfce-de1a0b6cf353"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/3611 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5c162f7591a547f4a8d79b7a592cf706"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No label_names provided for model class `PeftModelForSeq2SeqLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='407' max='407' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [407/407 02:30, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.881900</td>\n",
              "      <td>0.637854</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "WARNING:__main__:Parameter found on cuda:0, moving to cuda\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 0: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 1: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 2: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 3: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 4: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 5: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 6: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 7: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 8: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 9: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 10: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 11: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 12: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 13: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 14: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 15: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 16: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 17: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 18: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 19: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 20: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 21: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 22: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 23: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 24: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 25: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 26: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 27: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 28: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 29: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 30: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 31: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 32: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 33: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 34: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 35: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 36: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 37: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 38: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 39: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 40: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 41: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 42: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 43: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 44: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 45: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 46: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 47: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 48: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 49: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 50: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 51: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 52: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 53: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 54: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 55: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 56: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 57: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 58: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 59: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 60: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 61: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 62: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 63: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 64: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 65: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 66: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 67: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 68: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 69: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 70: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 71: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 72: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 73: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 74: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 75: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 76: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 77: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 78: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 79: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 80: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 81: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 82: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 83: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 84: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 85: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 86: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 87: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 88: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 89: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 90: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 91: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 92: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 93: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 94: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 95: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 96: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 97: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 98: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 99: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 100: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 101: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 102: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 103: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 104: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 105: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 106: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 107: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 108: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 109: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 110: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 111: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 112: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 113: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 114: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 115: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 116: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 117: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 118: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 119: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 120: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 121: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 122: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 123: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 124: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 125: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 126: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 127: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 128: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 129: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 130: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 131: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 132: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 133: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 134: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 135: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 136: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 137: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 138: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 139: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 140: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 141: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 142: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 143: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 144: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 145: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 146: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 147: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 148: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 149: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 150: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 151: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 152: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 153: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 154: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 155: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 156: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 157: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 158: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 159: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 160: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 161: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 162: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 163: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 164: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 165: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 166: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 167: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 168: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 169: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 170: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 171: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 172: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 173: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 174: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 175: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 176: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 177: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 178: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 179: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 180: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 181: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 182: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 183: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 184: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 185: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 186: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 187: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 188: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 189: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 190: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 191: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 192: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 193: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 194: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 195: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 196: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 197: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 198: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 199: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 200: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 201: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 202: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 203: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 204: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 205: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 206: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 207: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 208: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 209: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 210: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 211: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 212: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 213: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 214: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 215: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 216: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 217: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 218: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 219: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 220: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 221: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 222: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 223: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 224: Input tensor input_ids device mismatch\n",
            "ERROR:__main__:Input tensor input_ids is on cuda:0, expected cuda\n",
            "ERROR:__main__:Error processing batch 225: Input tensor input_ids device mismatch\n",
            "WARNING:__main__:No valid prediction-reference pairs for BLEU score\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check tokenized data\n",
        "!head -n 2 /content/drive/MyDrive/llama_processed/llama_tokenized.jsonl\n",
        "\n",
        "# Expected output (example):\n",
        "#{\"normalized_text\": \"What is the total number of students?\", \"db_id\": \"students_db\"}\n",
        "\n",
        "# Check schema data\n",
        "!head -n 2 /content/drive/MyDrive/schema_linked/arabic_sql_with_schema.jsonl\n",
        "\n",
        "# Expected output (example):\n",
        "#{\"db_id\": \"students_db\", \"schema_prompt\": \"Table students has columns id, name, grade\", \"sql_query\": \"SELECT COUNT(*) FROM students;\"}\n",
        "#{\"normalized_text\": \"What is the total number of students?\", \"db_id\": \"students_db\"}\n",
        "\n",
        "# Check schema data\n",
        "#head -n 2 /content/drive/MyDrive/schema_linked/arabic_sql_with_schema.jsonl\n",
        "\n",
        "# Expected output (example):\n",
        "#{\"db_id\": \"students_db\", \"schema_prompt\": \"Table students has columns id, name, grade\", \"sql_query\": \"SELECT COUNT(*) FROM students;\"}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHADBwycFH3b",
        "outputId": "5898b72d-1748-42a2-fe82-161281127355"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"id\": \"2b406516-12f9-47a4-8ad7-d39f915c67ec\", \"original_text\": \"كم عدد رؤساء الأقسام الذين تزيد أعمارهم عن 56 سنة؟\", \"normalized_text\": \"كم عدد رءساء الءقسام الذين تزيد ءعمارهم عن 56 سنة؟\", \"token_ids\": [1, 29871, 30283, 30159, 29871, 30218, 30172, 30172, 29871, 30156, 30992, 30198, 30112, 30992, 24508, 30992, 30265, 30198, 30112, 30159, 24508, 30851, 30163, 30162, 29871, 30195, 30295, 30163, 30172, 29871, 30992, 30218, 30159, 30112, 30156, 30204, 30159, 29871, 30218, 30162, 29871, 29945, 29953, 29871, 30198, 30162, 30242, 219, 162], \"db_id\": \"department_management\", \"question\": \"How many heads of the departments are older than 56 ?\"}\n",
            "{\"id\": \"2970f0cb-68b9-4437-acd6-4ed38c105f79\", \"original_text\": \"اعرض قائمة بأسماء رؤساء الأقسام، مكان ميلادهم، وأعمارهم مرتبة حسب العمر.\", \"normalized_text\": \"اعرض قاءمة بءسماء رءساء الءقسام، مكان ميلادهم، وءعمارهم مرتبة حسب العمر.\", \"token_ids\": [1, 29871, 30112, 30218, 30156, 30624, 29871, 30265, 30112, 30992, 30159, 30242, 29871, 30177, 30992, 30198, 30159, 30112, 30992, 29871, 30156, 30992, 30198, 30112, 30992, 24508, 30992, 30265, 30198, 30112, 30159, 31116, 29871, 30159, 30283, 30112, 30162, 29871, 30159, 30163, 30138, 30112, 30172, 30204, 30159, 31116, 29871, 30171, 30992, 30218, 30159, 30112, 30156, 30204, 30159, 29871, 30159, 30156, 30195, 30177, 30242, 29871, 30240, 30198, 30177, 24508, 30218, 30159, 30156, 29889], \"db_id\": \"department_management\", \"question\": \"List the name, born state and age of the heads of departments ordered by age.\"}\n",
            "{\"db_id\": \"department_management\", \"schema_prompt\": \"Table `department` has columns: CREATE, PRIMARY\\nTable `head` has columns: CREATE, PRIMARY\\nTable `management` has columns: CREATE, PRIMARY, FOREIGN, FOREIGN\", \"input_prompt\": \"Given the following database schema:\\nTable `department` has columns: CREATE, PRIMARY\\nTable `head` has columns: CREATE, PRIMARY\\nTable `management` has columns: CREATE, PRIMARY, FOREIGN, FOREIGN\\n\\nGenerate SQL for this Arabic question:\\nكم عدد رؤساء الأقسام الذين تزيد أعمارهم عن 56 سنة؟\", \"original_question\": \"How many heads of the departments are older than 56 ?\", \"arabic_question\": \"كم عدد رؤساء الأقسام الذين تزيد أعمارهم عن 56 سنة؟\", \"sql_query\": \"SELECT count(*) FROM head WHERE age  >  56\"}\n",
            "{\"db_id\": \"department_management\", \"schema_prompt\": \"Table `department` has columns: CREATE, PRIMARY\\nTable `head` has columns: CREATE, PRIMARY\\nTable `management` has columns: CREATE, PRIMARY, FOREIGN, FOREIGN\", \"input_prompt\": \"Given the following database schema:\\nTable `department` has columns: CREATE, PRIMARY\\nTable `head` has columns: CREATE, PRIMARY\\nTable `management` has columns: CREATE, PRIMARY, FOREIGN, FOREIGN\\n\\nGenerate SQL for this Arabic question:\\nاعرض قائمة بأسماء رؤساء الأقسام، مكان ميلادهم، وأعمارهم مرتبة حسب العمر.\", \"original_question\": \"List the name, born state and age of the heads of departments ordered by age.\", \"arabic_question\": \"اعرض قائمة بأسماء رؤساء الأقسام، مكان ميلادهم، وأعمارهم مرتبة حسب العمر.\", \"sql_query\": \"SELECT name ,  born_state ,  age FROM head ORDER BY age\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "schema_file = \"/content/drive/MyDrive/schema_linked/arabic_sql_with_schema.jsonl\"\n",
        "\n",
        "with open(schema_file, 'r', encoding='utf-8') as f:\n",
        "    for line_number, line in enumerate(f, 1):\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "        try:\n",
        "            data = json.loads(line)\n",
        "            if \"sql_query\" not in data or not data[\"sql_query\"]:\n",
        "                print(f\"Line {line_number} missing sql_query\")\n",
        "            elif \"db_id\" not in data and \"id\" not in data:\n",
        "                print(f\"Line {line_number} missing db_id or id\")\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"Line {line_number} invalid JSON: {str(e)}\")"
      ],
      "metadata": {
        "id": "XdeO8R-IDNMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import logging\n",
        "from pathlib import Path\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Paths\n",
        "DATABASE_ROOT = \"/content/data/database\"\n",
        "QUERIES_DIR = \"/content/data/queries\"\n",
        "\n",
        "def extract_first_table_name(schema_content):\n",
        "    \"\"\"Extract the first table name from schema content\"\"\"\n",
        "    # Match CREATE TABLE statements\n",
        "    table_match = re.search(r'CREATE\\s+TABLE\\s+(?:IF\\s+NOT\\s+EXISTS\\s+)?[\"`]?(\\w+)[\"`]?',\n",
        "                          schema_content, re.IGNORECASE)\n",
        "    if table_match:\n",
        "        return table_match.group(1)\n",
        "    return None\n",
        "\n",
        "def generate_sample_query(table_name):\n",
        "    \"\"\"Generate a basic SELECT query for the table\"\"\"\n",
        "    if not table_name:\n",
        "        return \"-- No valid table found\"\n",
        "    return f\"SELECT * FROM {table_name} LIMIT 1;\\n-- Add your actual SQL query here\"\n",
        "\n",
        "def process_all_databases():\n",
        "    \"\"\"Process all database folders to generate query files\"\"\"\n",
        "    os.makedirs(QUERIES_DIR, exist_ok=True)\n",
        "    logger.info(f\"Created queries directory: {QUERIES_DIR}\")\n",
        "\n",
        "    processed = 0\n",
        "    skipped = 0\n",
        "\n",
        "    # Process each database folder\n",
        "    for db_id in os.listdir(DATABASE_ROOT):\n",
        "        db_path = os.path.join(DATABASE_ROOT, db_id)\n",
        "\n",
        "        # Skip non-directories\n",
        "        if not os.path.isdir(db_path):\n",
        "            continue\n",
        "\n",
        "        schema_path = os.path.join(db_path, \"schema.sql\")\n",
        "\n",
        "        # Skip if schema.sql doesn't exist\n",
        "        if not os.path.exists(schema_path):\n",
        "            logger.warning(f\"Missing schema.sql for {db_id}\")\n",
        "            skipped += 1\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            # Read schema.sql\n",
        "            with open(schema_path, 'r', encoding='utf-8') as f:\n",
        "                schema_content = f.read()\n",
        "\n",
        "            # Extract first table name\n",
        "            table_name = extract_first_table_name(schema_content)\n",
        "\n",
        "            # Generate sample SQL query\n",
        "            sample_sql = generate_sample_query(table_name)\n",
        "\n",
        "            # Write to queries directory\n",
        "            query_path = os.path.join(QUERIES_DIR, f\"{db_id}.sql\")\n",
        "            with open(query_path, 'w', encoding='utf-8') as f:\n",
        "                f.write(sample_sql)\n",
        "\n",
        "            processed += 1\n",
        "            if processed % 10 == 0:\n",
        "                logger.info(f\"Processed {processed} databases...\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error processing {db_id}: {str(e)}\")\n",
        "            skipped += 1\n",
        "\n",
        "    logger.info(f\"\\n🎉 Generated query files for {processed} databases\")\n",
        "    if skipped > 0:\n",
        "        logger.warning(f\"⚠️ Skipped {skipped} databases due to missing schema.sql or errors\")\n",
        "\n",
        "# Run the processor\n",
        "process_all_databases()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Laklcu2n_WhB",
        "outputId": "db47284a-89a0-4342-d508-30880be711a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Missing schema.sql for wine_1\n",
            "WARNING:__main__:Missing schema.sql for icfp_1\n",
            "WARNING:__main__:Missing schema.sql for epinions_1\n",
            "WARNING:__main__:Missing schema.sql for flight_4\n",
            "WARNING:__main__:Missing schema.sql for car_1\n",
            "WARNING:__main__:Missing schema.sql for chinook_1\n",
            "WARNING:__main__:Missing schema.sql for world_1\n",
            "WARNING:__main__:Missing schema.sql for wta_1\n",
            "WARNING:__main__:Missing schema.sql for formula_1\n",
            "WARNING:__main__:Missing schema.sql for twitter_1\n",
            "WARNING:__main__:Missing schema.sql for inn_1\n",
            "WARNING:__main__:Missing schema.sql for company_1\n",
            "WARNING:__main__:Missing schema.sql for voter_1\n",
            "WARNING:__main__:Missing schema.sql for student_1\n",
            "WARNING:__main__:Missing schema.sql for flight_2\n",
            "WARNING:__main__:Missing schema.sql for college_1\n",
            "WARNING:__main__:Missing schema.sql for small_bank_1\n",
            "WARNING:__main__:Missing schema.sql for college_2\n",
            "WARNING:__main__:⚠️ Skipped 18 databases due to missing schema.sql or errors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_database_folders():\n",
        "    \"\"\"Process all database folders and generate schema-linked data with SQL queries\"\"\"\n",
        "    total_processed = 0\n",
        "    missing_schemas = 0\n",
        "    missing_queries = 0\n",
        "\n",
        "    query_dir = \"/content/data/queries\"\n",
        "\n",
        "    with open(output_file, \"w\", encoding=\"utf-8\") as out_f:\n",
        "        for db_folder in os.listdir(database_root):\n",
        "            folder_path = os.path.join(database_root, db_folder)\n",
        "            if not os.path.isdir(folder_path):\n",
        "                continue\n",
        "\n",
        "            schema_path = os.path.join(folder_path, \"schema.sql\")\n",
        "            query_path = os.path.join(query_dir, f\"{db_folder}.sql\")\n",
        "\n",
        "            if not os.path.exists(schema_path):\n",
        "                missing_schemas += 1\n",
        "                print(f\"⚠️ Missing schema for {db_folder}\")\n",
        "                continue\n",
        "\n",
        "            if not os.path.exists(query_path):\n",
        "                missing_queries += 1\n",
        "                print(f\"⚠️ Missing query for {db_folder}\")\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                with open(schema_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                    sql_content = f.read()\n",
        "\n",
        "                with open(query_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                    sql_query = f.read().strip()\n",
        "\n",
        "                schema_data = extract_schema_elements(sql_content)\n",
        "                schema_prompt = format_schema_prompt(schema_data)\n",
        "\n",
        "                # Write to JSONL\n",
        "                out_f.write(json.dumps({\n",
        "                    \"db_id\": db_folder,\n",
        "                    \"schema_prompt\": schema_prompt,\n",
        "                    \"sql_query\": sql_query  # ✅ Include SQL query\n",
        "                }, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "                total_processed += 1\n",
        "                if total_processed % 10 == 0:\n",
        "                    print(f\"✅ Processed {total_processed} databases...\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error processing {db_folder}: {str(e)}\")\n",
        "\n",
        "    print(f\"\\n🎉 Done: Processed {total_processed} databases\")\n",
        "    print(f\"🚫 Missing schema files: {missing_schemas}\")\n",
        "    print(f\"🚫 Missing query files: {missing_queries}\")"
      ],
      "metadata": {
        "id": "c1FeWZdCBnPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "file_path = \"/content/drive/MyDrive/schema_linked/arabic_sql_with_schema.jsonl\"\n",
        "\n",
        "def check_jsonl_file(file_path):\n",
        "    if not os.path.exists(file_path):\n",
        "        print(f\"❌ File not found: {file_path}\")\n",
        "        return\n",
        "\n",
        "    print(f\"🔍 Checking file: {file_path}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    line_count = 0\n",
        "    valid_lines = 0\n",
        "    invalid_lines = []\n",
        "\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line_number, line in enumerate(f, 1):\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "\n",
        "            line_count += 1\n",
        "\n",
        "            try:\n",
        "                data = json.loads(line)\n",
        "                required_fields = ['db_id', 'schema_prompt', 'sql_query']\n",
        "                if all(field in data for field in required_fields):\n",
        "                    valid_lines += 1\n",
        "                else:\n",
        "                    missing = [f for f in required_fields if f not in data]\n",
        "                    print(f\"⚠️ Line {line_number} missing fields: {missing}\")\n",
        "            except json.JSONDecodeError as e:\n",
        "                invalid_lines.append((line_number, line, str(e)))\n",
        "\n",
        "    print(f\"\\n📊 Summary:\")\n",
        "    print(f\"Total lines: {line_count}\")\n",
        "    print(f\"Valid JSON lines: {valid_lines}\")\n",
        "    print(f\"Invalid lines: {len(invalid_lines)}\")\n",
        "\n",
        "check_jsonl_file(file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gn53e8FCBybd",
        "outputId": "63528025-322b-46db-db98-2135a1653d81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 Checking file: /content/drive/MyDrive/schema_linked/arabic_sql_with_schema.jsonl\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "📊 Summary:\n",
            "Total lines: 6396\n",
            "Valid JSON lines: 6396\n",
            "Invalid lines: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tokenized and schema data\n",
        "tokenized_data = load_data(args.tokenized_data)\n",
        "schema_data = load_data(args.schema_data) if os.path.exists(args.schema_data) else None\n",
        "if not schema_data:\n",
        "    logger.warning(\"No schema data provided. Using tokenized data for testing only.\")"
      ],
      "metadata": {
        "id": "Wh0vLwXCh4Nv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xjpU9QXV1SYW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}